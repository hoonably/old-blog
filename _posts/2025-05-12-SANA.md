---
layout: blog
title: "SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers"
subtitle: ""
date: 2025-05-12 15:48:23 +09:00
categories: Paper
author: "hoonably"
---
<div class="page-body"><table id="1b2451cf-7b79-80ff-ab8a-f57c4f2f327b" class="simple-table"><tbody><tr id="1b2451cf-7b79-8083-bf78-d343f537ecac"><th id="MApI" class="simple-table-header-color simple-table-header" style="width:125.5px">ArXiv</th><td id="L|H:" class="" style="width:580.5px"><a href="https://arxiv.org/abs/2410.10629">https://arxiv.org/abs/2410.10629</a></td></tr><tr id="1b2451cf-7b79-801e-935d-e0aa93b0292b"><th id="MApI" class="simple-table-header-color simple-table-header" style="width:125.5px">Project Page</th><td id="L|H:" class="" style="width:580.5px"><a href="https://nvlabs.github.io/Sana/">https://nvlabs.github.io/Sana/</a></td></tr><tr id="1b2451cf-7b79-806b-b5fa-da2d3db4b742"><th id="MApI" class="simple-table-header-color simple-table-header" style="width:125.5px">Github Code</th><td id="L|H:" class="" style="width:580.5px"><a href="https://github.com/NVlabs/Sana">https://github.com/NVlabs/Sana</a></td></tr></tbody></table><figure class="block-color-teal_background callout" style="white-space:pre-wrap;display:flex" id="1f1451cf-7b79-80e5-8185-cf1698a2fcae"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="1f1451cf-7b79-8090-9e98-f4d030d3ed93" class=""><strong>Key Differentiator</strong></p><ol type="1" id="1f1451cf-7b79-80d3-b805-edbbf636523d" class="numbered-list" start="1"><li>Efficient Linear DiT design<p id="1f1451cf-7b79-807c-b8c8-ed9a8e3e77de" class="">ReLU 기반 Linear Attention 도입</p><p id="1f1451cf-7b79-8035-9fb3-fd496ef48c75" class="">Mix-FFN Block</p></li></ol><ol type="1" id="1f1451cf-7b79-8027-8574-dea4713ff7e4" class="numbered-list" start="2"><li>Deep Compression Autoencoder<p id="1f1451cf-7b79-8031-bfbb-ce92ad7de303" class="">→ 이로 인한 32배 압축 가능으로 연산도 빨라짐</p></li></ol></div></figure><figure id="1f1451cf-7b79-8089-849d-c3ec70ac8797"><div class="source"><a href="/files/2025-05-12-SANA/250501_JeonghoonPark_SANA_Efficient_High-Resolution.pdf">250501_JeonghoonPark_SANA_Efficient_High-Resolution.pdf</a></div></figure><figure id="1f1451cf-7b79-8024-8298-e87fc512a984"><div class="source"><a href="/files/2025-05-12-SANA/250501_JeonghoonPark_SANA_Efficient_High-Resolution.pptx">250501_JeonghoonPark_SANA_Efficient_High-Resolution.pptx</a></div></figure><p id="1b2451cf-7b79-8051-a192-eaabad839b2f" class="">
</p><figure id="1c0451cf-7b79-8009-b100-e7fde0ee7305" class="image"><a href="/files/2025-05-12-SANA/image.png"><img style="width:336px" src="/files/2025-05-12-SANA/image.png"/></a></figure><p id="1e1451cf-7b79-8065-ba44-fffe93414c27" class="">
</p><p id="1e1451cf-7b79-8017-b604-d5ac7746ae95" class="">이번에는 SVDQuant의 저자인 Song Han이 또 일을 냈다.</p><p id="1e1451cf-7b79-80cd-8a35-c29149cbc47a" class="">SANA 라는 Diffusion 모델을 NVIDIA에서 제작했는데, 역대급이다.</p><p id="1e1451cf-7b79-80d2-9a60-efd89be7917b" class="">내가 하려던 On-device 4K Diffusion 연구에도 크게 도움될 것 같아서 읽어보았다.</p><p id="1e1451cf-7b79-8068-a57d-cdded33b9d3f" class="">
</p><p id="1e1451cf-7b79-809c-8b6e-e89ac9a566b4" class="">
</p><h1 id="1b9451cf-7b79-80c6-9476-ccc63742e7c3" class="">1. Introduction</h1><p id="1b9451cf-7b79-80b6-bbc6-f714e85ebac1" class="">지난 1년동안 Diffusion 모델은 text-to-image 연구에서 상당한 진전을 보임.</p><p id="1b9451cf-7b79-800c-801e-cf59e063ce7f" class="">하지만, 아래와 같이 상업 모델은 파라미터가 매우 커짐 → 높은 학습 및 추론 비용을 초래하여 비용이 많이 들음.</p><blockquote id="1b9451cf-7b79-80c6-9a00-d8a6af96f4da" class="">Industry models are becoming increasingly large, with parameter counts escalating from PixArt’s 0.6B parameters to SD3 at 8B, LiDiT at 10B, Flux at 12B, and Playground v3 at 24B.</blockquote><p id="1b9451cf-7b79-80c2-919a-f454fca1b6e7" class=""> cloud 뿐만 아니라 edge devices에서도 빠르게 실행되는 고해상도 image generator를 개발할 수 없을까?</p><p id="1b9451cf-7b79-8074-ac62-cf738cb861d3" class="">
</p><figure id="1b9451cf-7b79-8055-beb8-cf3ac6c01b58" class="image"><a href="/files/2025-05-12-SANA/image%201.png"><img style="width:709.9921875px" src="/files/2025-05-12-SANA/image%201.png"/></a></figure><p id="1b9451cf-7b79-802d-8047-f54d18790de3" class="">이 논문은 1024 × 1024 ~ 4096 × 4096 범위의 해상도에서 이미지를 효율적이고 비용 효율적으로 훈련하고 합성하도록 설계된 파이프 라인 인 SANA를 제안</p><p id="1b9451cf-7b79-8003-909f-fd18695a1fc9" class="block-color-orange_background">Pixart-σ (Chen et al., 2024a)를 제외하고는 4K 해상도 이미지 생성을 직접 탐색하지 못했습니다. 그러나 Pixart-σ는 4K 해상도에 가까운 이미지를 생성하는 것으로 제한되며 (3840 × 2160) 이러한 고해상도 이미지를 생성 할 때 비교적 느립니다. 이 야심 찬 목표를 달성하기 위해 몇 가지 핵심 디자인을 제안합니다.</p><p id="1b9451cf-7b79-80bc-adeb-e4e3551f01c6" class="">
</p><h1 id="1b9451cf-7b79-8047-8f49-d17177f41bab" class="">2. METHODS</h1><h2 id="1bb451cf-7b79-8048-82a5-e0315876b9de" class="">2.1 DEEP COMPRESSION AUTOENCODER</h2><h3 id="1bb451cf-7b79-80cb-ad09-f5479498ed54" class="">2.1.1 PRELIMINARY</h3><p id="1bb451cf-7b79-804a-ac0a-eebc5d2259cb" class="">원래 diffusion 모델은 이미지 픽셀 공간 (pixel space) 위에서 직접 작동 → 훈련, 추론 둘다 너무 느리고 무거움</p><p id="1e1451cf-7b79-80ad-aac2-cd93e0a517c4" class="">
</p><p id="1e1451cf-7b79-8038-9550-f5738fafe434" class=""><strong>Latent Diffusion Models</strong></p><p id="1e1451cf-7b79-8093-bf1a-f9ff909ebe01" class="">Autoencoder로 이미지 압축 후 압축된 latent 공간 위에서 diffusion을 돌리자!</p><p id="1e1451cf-7b79-80cb-9425-d0d88bebca42" class="">→ 8배 압축 사용</p><figure id="1e1451cf-7b79-80b1-97bf-d94788f041a8" class="image"><a href="/files/2025-05-12-SANA/image%202.png"><img style="width:336px" src="/files/2025-05-12-SANA/image%202.png"/></a></figure><p id="1e1451cf-7b79-80a9-af4b-f26cfc1ad23e" class="">
</p><p id="1e1451cf-7b79-8025-89a9-f25430858d86" class="">
</p><p id="1e1451cf-7b79-80ff-9148-cfcd4f64f67e" class=""><strong>Diffusion Transformer (DiT)</strong></p><p id="1e1451cf-7b79-80f1-9e7d-d4ab130b2381" class="">추가로 latent feature를 Patch 단위로 또 나눠서 처리</p><p id="1b9451cf-7b79-8098-9193-e586022348f2" class="">패치크기가 PxP 라면 최종적으로 다루는 토큰 개수는</p><figure id="1e1451cf-7b79-805c-8524-fa185c764a01" class="image" style="text-align:left"><a href="/files/2025-05-12-SANA/image%203.png"><img style="width:144px" src="/files/2025-05-12-SANA/image%203.png"/></a></figure><p id="1e1451cf-7b79-806c-a6f8-cbb7ca631e88" class="">
</p><p id="1e1451cf-7b79-80c0-8075-cc5ee94161f7" class="">기존 latent diffusion 모델들(PixArt, SD3, Flux 등)은 보통 다음 세팅을 씀</p><ul id="1e1451cf-7b79-80c4-a8f3-c6df52d2fd09" class="bulleted-list"><li style="list-style-type:disc"><strong>AE-F8C4P2</strong> 또는 <strong>AE-F8C16P2</strong><ul id="1e1451cf-7b79-80a9-9429-f1cf0d2d2330" class="bulleted-list"><li style="list-style-type:circle"><strong>F8</strong>: Autoencoder가 8배 압축</li></ul><ul id="1e1451cf-7b79-8001-bad1-c21f042864fc" class="bulleted-list"><li style="list-style-type:circle"><strong>C4</strong> 또는 <strong>C16</strong>: latent 채널 수 (4개나 16개)</li></ul><ul id="1e1451cf-7b79-807d-96f5-e8117ce8331e" class="bulleted-list"><li style="list-style-type:circle"><strong>P2</strong>: Patch 크기 2×2로 묶기</li></ul></li></ul><p id="1b9451cf-7b79-8061-b473-ca4c6cebf9ec" class="">
</p><p id="1e1451cf-7b79-80c9-8b47-c9263e77a942" class="">기존처럼 8배 압축만 하면 계산량이 여전히 너무 많음</p><p id="1e1451cf-7b79-80fb-ba54-e86186ec4dea" class="">그래서 SANA는 과감하게 <strong>32배 압축(AE-F32)</strong>하고, 패치로는 묶지 않음.</p><p id="1e1451cf-7b79-80f8-a5b4-ccb66a1d1e24" class="">
</p><h3 id="1e1451cf-7b79-8085-acf1-c48b62023c68" class="">2.1.2 AUTOENCODER DESIGN PHILOSOPHY</h3><table id="1e1451cf-7b79-80c4-bc4e-e1bc262ed3ab" class="simple-table"><tbody><tr id="1e1451cf-7b79-80e3-83ee-dffd0bf9e6d2"><td id="`J=h" class="" style="width:230px">구분</td><td id="`bRS" class="" style="width:230px">기존 (PixArt, Flux)</td><td id="Ir}k" class="" style="width:230px">SANA</td></tr><tr id="1e1451cf-7b79-80e2-a661-de1049a8f47a"><td id="`J=h" class="" style="width:230px">AE 압축비</td><td id="`bRS" class="" style="width:230px">8배 (F=8)</td><td id="Ir}k" class="" style="width:230px">32배 (F=32)</td></tr><tr id="1e1451cf-7b79-80f2-92b8-ee193e976609"><td id="`J=h" class="" style="width:230px">Patchify (P=2)</td><td id="`bRS" class="" style="width:230px">O (패치로 묶음)</td><td id="Ir}k" class="" style="width:230px">✖️ (패치 안 묶음)</td></tr><tr id="1e1451cf-7b79-8049-9e56-e39dfbc57552"><td id="`J=h" class="" style="width:230px">최종 Token 수</td><td id="`bRS" class="" style="width:230px">줄였지만 아직 많음</td><td id="Ir}k" class="" style="width:230px">훨씬 적음 (16배 감소)</td></tr></tbody></table><figure id="1e1451cf-7b79-8045-a540-cda5274fd13c" class="image"><a href="/files/2025-05-12-SANA/image%204.png"><img style="width:432px" src="/files/2025-05-12-SANA/image%204.png"/></a></figure><p id="1e1451cf-7b79-80b0-ad98-dffcd03267c4" class="">위의 표를 보면 알 수 있듯이 32배 압축하더라도 점수가 크게 떨어지지 않는 모습을 보임</p><p id="1e1451cf-7b79-8081-b043-db7b043be64a" class="">
</p><h3 id="1e1451cf-7b79-8092-8255-d8e0b073f06d" class="">2.1.3 ABLATION OF AUTOENCODER DESIGNS</h3><ul id="1e1451cf-7b79-805c-baa3-e78917da4b2b" class="bulleted-list"><li style="list-style-type:disc"><strong>어디서 압축을 더 하는 게 좋은가? (AE vs DiT)</strong></li></ul><div id="1e1451cf-7b79-80cd-8673-ca56ffeba583" class="column-list"><div id="1e1451cf-7b79-8040-9327-e9af8f5fa806" style="width:50%" class="column"><figure id="1e1451cf-7b79-8013-9275-e07b37b17ec6" class="image"><a href="/files/2025-05-12-SANA/image%205.png"><img style="width:384px" src="/files/2025-05-12-SANA/image%205.png"/></a></figure></div><div id="1e1451cf-7b79-800b-a3e1-d9b700437cab" style="width:50%" class="column"><table id="1e1451cf-7b79-8065-800d-c5144119a97c" class="simple-table"><tbody><tr id="1e1451cf-7b79-801c-9ad0-ca35952be937"><td id="G`wd" class="" style="width:125px">설정</td><td id="GEiu" class="" style="width:252px">설명</td></tr><tr id="1e1451cf-7b79-806f-bf4c-ce61d69ce563"><td id="G`wd" class="" style="width:125px">AE-F8C16P4</td><td id="GEiu" class="" style="width:252px">8배 압축 + 패치 크기 4</td></tr><tr id="1e1451cf-7b79-8076-9db3-e52d727bc6c6"><td id="G`wd" class="" style="width:125px">AE-F16C32P2</td><td id="GEiu" class="" style="width:252px">16배 압축 + 패치 크기 2</td></tr><tr id="1e1451cf-7b79-80c5-af81-e6bf4a0e97af"><td id="G`wd" class="" style="width:125px">AE-F32C32P1</td><td id="GEiu" class="" style="width:252px">32배 압축 + 패치 사용하지 않음 (SANA)</td></tr></tbody></table></div></div><p id="1e1451cf-7b79-804d-883e-cbead4e38a14" class="">AE-F32C32P1 설정이 가장 뛰어난 성능(FID, CLIP Score)을 기록</p><p id="1e1451cf-7b79-8076-a4e1-dd378d999616" class="">Autoencoder가 압축을 전적으로 담당하는 것이 성능 및 훈련 안정성 모두에서 가장 우수</p><p id="1e1451cf-7b79-80c1-8a75-c172c62ee7c6" class="">
</p><ul id="1e1451cf-7b79-80ce-8275-e836b7364d37" class="bulleted-list"><li style="list-style-type:disc"><strong>Autoencoder latent 채널 수를 몇 개로 하는 게 좋은가?</strong></li></ul><div id="1e1451cf-7b79-808a-ba25-e8ce8eb7085a" class="column-list"><div id="1e1451cf-7b79-800a-85f2-e521ccbcff47" style="width:50%" class="column"><figure id="1e1451cf-7b79-80c7-a890-ddb81f6586fc" class="image"><a href="/files/2025-05-12-SANA/image%206.png"><img style="width:709.9921875px" src="/files/2025-05-12-SANA/image%206.png"/></a></figure></div><div id="1e1451cf-7b79-80eb-837c-c09fe3a2a8c6" style="width:50%" class="column"><ul id="1e1451cf-7b79-80d3-9f0b-db0256b9b34b" class="bulleted-list"><li style="list-style-type:disc">C=16, C=32, C=64 실험 수행</li></ul><ul id="1e1451cf-7b79-802c-8481-e09e4bc919b6" class="bulleted-list"><li style="list-style-type:disc">C=16은 정보 손실로 인해 품질 저하 발생</li></ul><ul id="1e1451cf-7b79-80ff-a1cc-cbca8878281f" class="bulleted-list"><li style="list-style-type:disc">C=64는 복원 품질은 좋았으나 모델 복잡도가 급격히 증가하여 비효율적임</li></ul><ul id="1e1451cf-7b79-808b-8721-d12f9578fa35" class="bulleted-list"><li style="list-style-type:disc">C=32가 성능과 효율 사이에서 최적 균형을 달성함</li></ul><p id="1e1451cf-7b79-80cf-92a7-eae55efd12c9" class="">
</p></div></div><p id="1e1451cf-7b79-8077-8e8b-da06c8fd118a" class="">
</p><h2 id="1e1451cf-7b79-8005-9906-fe8ee9f67931" class="">2.2 EFFICIENT LINEAR DIT DESIGN</h2><ul id="1e1451cf-7b79-807b-9f8c-db81db50d72e" class="bulleted-list"><li style="list-style-type:disc">기존 diffusion transformer(예: DiT) 구조는 <strong>Self-Attention</strong>을 사용함.</li></ul><ul id="1e1451cf-7b79-80a9-8648-d54890f987bb" class="bulleted-list"><li style="list-style-type:disc">Self-Attention의 연산량은 <strong>O(N²)</strong> 에 비례함.<ul id="1e1451cf-7b79-80a5-9397-cbd64ebb1b21" class="bulleted-list"><li style="list-style-type:circle">NNN은 입력 토큰 수</li></ul><ul id="1e1451cf-7b79-8036-b868-d42c93c4bc54" class="bulleted-list"><li style="list-style-type:circle">토큰 수가 많아지면 연산량이 급격히 커짐</li></ul></li></ul><ul id="1e1451cf-7b79-80b3-a080-e264150bd729" class="bulleted-list"><li style="list-style-type:disc">4K 해상도 이미지를 다루려면, latent token 수가 많아질 수밖에 없음.</li></ul><p id="1e1451cf-7b79-80bb-9674-ec05fe71520b" class="">→ 이때, 기존 연구들은 이 문제를 해결하려고 해상도 낮추거나 Token 수를 줄였음.</p><p id="1e1451cf-7b79-806c-a8f4-da4c638adbc7" class="">
</p><p id="1e1451cf-7b79-808e-aed5-dee3bacf859a" class="">
</p><h3 id="1e1451cf-7b79-8005-bbf9-ca3630372b5f" class="">ReLU 기반 Linear Attention 도입</h3><p id="1e1451cf-7b79-80a5-bd1f-fe258c3e21d3" class="">기존 Softmax 기반 Attention을 제거하고, <strong>ReLU를 이용한 Linear Attention</strong>을 채택</p><p id="1e1451cf-7b79-8040-8427-e6db28bd3801" class="">
</p><figure id="1e1451cf-7b79-8012-8b9a-d691ed4b9550" class="image" style="text-align:left"><a href="/files/2025-05-12-SANA/image%207.png"><img style="width:384px" src="/files/2025-05-12-SANA/image%207.png"/></a></figure><p id="1e1451cf-7b79-804e-b8d2-cd2ce2a7b811" class="">Softmax는 모든 Query-Token 조합을 다 계산하기 때문에 O(N²) 복잡도가 발생</p><p id="1e1451cf-7b79-80ef-a096-fe7b18a64df7" class="">
</p><div id="1e1451cf-7b79-8037-ab88-f625c0cce9d4" class="column-list"><div id="1e1451cf-7b79-8087-91f2-eda7b8ba45ee" style="width:31.25%" class="column"><figure id="1e1451cf-7b79-809b-b6ba-d5b13e8c53f3" class="image"><a href="/files/2025-05-12-SANA/image%208.png"><img style="width:331.9921875px" src="/files/2025-05-12-SANA/image%208.png"/></a></figure></div><div id="1e1451cf-7b79-80c8-a5f7-e54f713aa754" style="width:68.75%" class="column"><ul id="1e1451cf-7b79-80c7-8c3e-d602057e8287" class="bulleted-list"><li style="list-style-type:disc">SANA에서는 다음처럼 계산 구조를 변경:<ol type="1" id="1e1451cf-7b79-8075-939d-fdd44b7c9e75" class="numbered-list" start="1"><li>각 Key에 ReLU를 적용함: <strong>ReLU(K)</strong></li></ol><ol type="1" id="1e1451cf-7b79-8090-8371-e394e722bbc9" class="numbered-list" start="2"><li>두 가지 공유 term을 미리 계산<figure id="1e1451cf-7b79-801c-aadc-efed4cd39a92" class="image"><a href="/files/2025-05-12-SANA/image%209.png"><img style="width:432px" src="/files/2025-05-12-SANA/image%209.png"/></a></figure></li></ol><ol type="1" id="1e1451cf-7b79-808e-b95f-c187fb75f285" class="numbered-list" start="3"><li>이후, 각 Query에 대해 이 pre-computed shared term을 재사용하여 Attention을 계산</li></ol></li></ul><ul id="1e1451cf-7b79-8045-a00d-d03540686623" class="bulleted-list"><li style="list-style-type:disc">이 방식은 Query마다 개별적으로 연산할 필요가 없어서, 전체 Attention 계산이 <strong>O(N)</strong> 으로 줄어듦.</li></ul><p id="1e1451cf-7b79-801f-b21f-f85a6a1000cf" class="">
</p></div></div><p id="1e1451cf-7b79-808f-b019-c49c74b52e7e" class="">
</p><p id="1e1451cf-7b79-80e2-9406-f12f81268cad" class=""><strong>PixArt도 Linear이랬는데 다른점은?</strong></p><p id="1e1451cf-7b79-8062-8136-e2c8084383eb" class="">PixArt에서는 Key와 Value 토큰을 압축하여 연산량을 줄여서 Engineering Optimization으로 O(N)과 비슷하게 하는 방식, SANA에서는 아예 수학적으로 계산량이 O(N)</p><p id="1e1451cf-7b79-8017-a3ee-f319d05ef866" class="">
</p><p id="1e1451cf-7b79-80df-931b-ed23d48991ef" class="">
</p><p id="1e1451cf-7b79-8064-a4e3-e31fc8cd4e4b" class="">
</p><p id="1e1451cf-7b79-80ef-a40f-fde33951f7cf" class="">
</p><h3 id="1e1451cf-7b79-8094-bd3c-c9de943dfcfa" class="">Mix-FFN Block</h3><div id="1e1451cf-7b79-8014-8b6b-e9dc846abe4b" class="column-list"><div id="1e1451cf-7b79-8057-94ea-e74d67472b3d" style="width:18.75%" class="column"><figure id="1e1451cf-7b79-80e5-9d1d-e73d7e7f40e8" class="image"><a href="/files/2025-05-12-SANA/image%2010.png"><img style="width:331.9921875px" src="/files/2025-05-12-SANA/image%2010.png"/></a></figure></div><div id="1e1451cf-7b79-80ab-a302-fe2daa822bdd" style="width:81.25%" class="column"><p id="1e1451cf-7b79-80a1-aa08-cea58de6dd01" class="">기존 Transformer의 FFN (Feed-Forward Network)은 단순히 2개의 Linear Layer로 구성되어 있었음.</p><p id="1e1451cf-7b79-80ef-b509-cc0b50e0f1c8" class="">FFN은 전역적인 정보는 잘 처리하지만, <strong>지역적인(local) 디테일</strong> 복원에는 약했음.</p><p id="1e1451cf-7b79-805d-9466-db4c3916938c" class="">
</p><p id="1e1451cf-7b79-80ec-9f84-df19eb48f269" class="">SANA의 해결책:</p><ul id="1e1451cf-7b79-80ff-93f1-db24e31feb9b" class="bulleted-list"><li style="list-style-type:disc">기존 MLP 사이에 <strong>3×3 Depthwise Convolution</strong>을 삽입</li></ul><ul id="1e1451cf-7b79-80de-ab5f-e1435a44ed76" class="bulleted-list"><li style="list-style-type:disc">이를 통해 지역 구조(local structure) 학습을 강화</li></ul><ul id="1e1451cf-7b79-80d0-b379-dd9435bfdda7" class="bulleted-list"><li style="list-style-type:disc">결과적으로 <strong>텍스처, 경계선</strong>, 이런 세밀한 부분 복원에 유리</li></ul><p id="1e1451cf-7b79-80c4-9e77-e9cabd2af50d" class="">
</p></div></div><p id="1e1451cf-7b79-80b8-bb4c-e665e1580398" class="">
</p><p id="1e1451cf-7b79-8006-a4dd-fe63c5ab7754" class="">
</p><p id="1e1451cf-7b79-8044-ac8d-e8338b9be66e" class="">
</p><h3 id="1e1451cf-7b79-80bb-890c-f14842ca751b" class="">DiT without Positional Encoding (NoPE)</h3><p id="1e1451cf-7b79-80ce-891a-fb72b793fcff" class="">기존 Transformer 구조는 입력 순서를 인식하도록 <strong>Positional Encoding</strong>을 사용했음.</p><p id="1e1451cf-7b79-8013-b598-c862c600b300" class="">왜냐면 Transformer는 입력 순서를 구별할 수 없었기 때문에…</p><p id="1e1451cf-7b79-8049-a242-d023119ca191" class="">
</p><p id="1e1451cf-7b79-8042-923b-c825f3c64f69" class="">하지만 4K 고해상도 latent처럼 토큰 수가 많을 때, Positional Encoding을 계산하고 저장하는 데도 비용이 큼.</p><p id="1e1451cf-7b79-8089-b32d-e572fc9e9972" class="">
</p><p id="1e1451cf-7b79-80d5-970e-fe47244c21e5" class="">SANA에서는 아예 Positional Encoding을 제거함</p><ul id="1e1451cf-7b79-8036-9c4b-e494430f2233" class="bulleted-list"><li style="list-style-type:disc"><strong>3×3 Depthwise Convolution</strong>이 Mix-FFN에 추가되어서 지역적 위치 관계를 학습할 수 있음.</li></ul><ul id="1e1451cf-7b79-80d1-8933-e737d1bd3c1b" class="bulleted-list"><li style="list-style-type:disc">Linear Attention은 <strong>전역 관계</strong>를 자연스럽게 포착할 수 있음.<p id="1e1451cf-7b79-809e-bddb-d0eaf31cdb41" class="">→ 별도로 위치 정보를 부여하지 않아도 충분히 패턴과 구조를 학습할 수 있음.</p></li></ul><p id="1e1451cf-7b79-8026-909d-cdad368961c3" class="">
</p><p id="1e1451cf-7b79-8019-8111-ce9a849dcee8" class="">결과적으로 품질이 유지되면서 구조가 간단해지고 메모리 연산량이 감소함</p><figure id="1e1451cf-7b79-8004-a692-db239ce26b72" class="image"><a href="/files/2025-05-12-SANA/image%2011.png"><img style="width:528px" src="/files/2025-05-12-SANA/image%2011.png"/></a></figure><p id="1e1451cf-7b79-8012-a554-e3dfe57ade45" class="">
</p><p id="1e1451cf-7b79-8081-90cc-f566df8cef16" class="">
</p><p id="1e1451cf-7b79-8099-beb2-e34547481389" class="">
</p><h3 id="1e1451cf-7b79-8065-8dc2-eb244c9483b7" class="">Triton Acceleration Training/Inference</h3><p id="1e1451cf-7b79-8009-8882-f72f1b0fb367" class="">Appendix에 추가한다고 되어있는데 아직 관련 내용 없음.</p><p id="1e1451cf-7b79-80f2-a05f-d5ba351d6134" class="">
</p><p id="1e1451cf-7b79-80a7-b828-dff44b07f9e3" class="">GPT가 알려준 내용</p><ul id="1e1451cf-7b79-8033-b1b7-e2d695b7ab99" class="bulleted-list"><li style="list-style-type:disc">Linear Attention을 구현할 때, 단순히 알고리즘만 개선하는 것으로는 부족함.</li></ul><ul id="1e1451cf-7b79-8077-a87e-c1336991bb9b" class="bulleted-list"><li style="list-style-type:disc">실제 연산 효율까지 극대화하려면, <strong>GPU kernel 레벨 최적화</strong>가 필요함.</li></ul><ul id="1e1451cf-7b79-803f-b28b-d781da695eb2" class="bulleted-list"><li style="list-style-type:disc">SANA는 <strong>Triton</strong>을 사용하여 Linear Attention 연산을 직접 최적화함.<ul id="1e1451cf-7b79-80b3-a15c-f707cb08f596" class="bulleted-list"><li style="list-style-type:circle">Triton은 NVIDIA가 지원하는 <strong>커스텀 GPU 커널 프로그래밍 프레임워크</strong>임.</li></ul><ul id="1e1451cf-7b79-80bc-82e3-f73e11445995" class="bulleted-list"><li style="list-style-type:circle">CUDA보다 단순한 문법으로, 고성능 커널을 작성할 수 있음.</li></ul></li></ul><p id="1e1451cf-7b79-80f5-9cf8-f5152a0df3ab" class="">
</p><p id="1e1451cf-7b79-8057-b111-cd1230ed2402" class="">Triton으로 최적화한 결과:</p><ul id="1e1451cf-7b79-8018-a807-d3e7e3ca22d3" class="bulleted-list"><li style="list-style-type:disc">Matrix 곱 연산(GEMM)과 Memory Access를 줄임.</li></ul><ul id="1e1451cf-7b79-80d4-9115-e14e7ee3c14e" class="bulleted-list"><li style="list-style-type:disc">실제 latency(지연 시간)와 memory bandwidth 소모를 크게 개선</li></ul><p id="1e1451cf-7b79-8052-a8a6-d0f4815a7773" class="">
</p><p id="1e1451cf-7b79-8088-8972-d06b7bf80def" class="">
</p><p id="1e1451cf-7b79-808b-b041-d57e8f2a3049" class="">
</p><p id="1e1451cf-7b79-80b5-8104-cba39f710b69" class="">
</p><p id="1e1451cf-7b79-80c8-8171-c1fecf5a0d15" class="">
</p><h2 id="1e1451cf-7b79-80bc-b5d2-de21611ef5ae" class="">2.3 TEXT ENCODER DESIGN</h2><p id="1e1451cf-7b79-805e-9efe-c9aaa7c50acd" class="">
</p><h3 id="1e1451cf-7b79-8096-9973-ccc984b429fa" class=""><strong>왜 T5 대신 Decoder-only LLM을 사용하는가?</strong></h3><p id="1e1451cf-7b79-80c0-880e-e483b616aa44" class="">
</p><p id="1e1451cf-7b79-8004-b481-fb568212a13f" class="">SANA는 Gemma를 Text Encoder로 사용하기로 채택</p><table id="1e1451cf-7b79-8033-bb10-eb08e0e4727d" class="simple-table"><tbody><tr id="1e1451cf-7b79-8098-bd99-ca73ab8a0411"><td id="XfwJ" class="" style="width:230px">항목</td><td id="sarK" class="" style="width:230px">기존 (T5)</td><td id="XAbw" class="" style="width:230px">SANA (Gemma-2)</td></tr><tr id="1e1451cf-7b79-808a-8fb8-f7e1898d4ba8"><td id="XfwJ" class="" style="width:230px">모델 구조</td><td id="sarK" class="" style="width:230px">Encoder-Decoder</td><td id="XAbw" class="" style="width:230px">Decoder-only</td></tr><tr id="1e1451cf-7b79-801f-b975-c1b949f0870e"><td id="XfwJ" class="" style="width:230px">Reasoning 능력</td><td id="sarK" class="" style="width:230px">제한적</td><td id="XAbw" class="" style="width:230px">매우 강함 (CoT, ICL 가능)</td></tr><tr id="1e1451cf-7b79-8026-9d08-f5268410c1d3"><td id="XfwJ" class="" style="width:230px">추론 속도</td><td id="sarK" class="" style="width:230px">느림 (T5-XXL)</td><td id="XAbw" class="" style="width:230px">6배 빠름 (Gemma-2-2B)</td></tr></tbody></table><p id="1e1451cf-7b79-806c-9e64-eb2f6cbd4ea1" class="">
</p><figure id="1e1451cf-7b79-807b-a441-e7801ac0efe7" class="image"><a href="/files/2025-05-12-SANA/image%2012.png"><img style="width:2054px" src="/files/2025-05-12-SANA/image%2012.png"/></a></figure><p id="1e1451cf-7b79-8000-90b4-ec164b0bde2f" class="">→ 빠른데도 불구하고, CLIP Score와 FID(이미지 품질 지표)에서는 <strong>성능이 비슷함</strong></p><p id="1e1451cf-7b79-809e-bd16-dc436c081112" class="">
</p><p id="1e1451cf-7b79-809d-83d5-fde23e843e83" class="">
</p><h3 id="1e1451cf-7b79-8050-936f-f4ceb386076a" class="">Decoder-only LLM을 Text Encoder로 쓰면서 생긴 문제 해결</h3><p id="1e1451cf-7b79-80a5-9bd6-e070176ae4d8" class="">Decoder-only LLM (Gemma, Qwen 등)의 텍스트 임베딩은 Variance가 훨씬 큼.</p><ul id="1e1451cf-7b79-8021-96ce-eee0265e24c2" class="bulleted-list"><li style="list-style-type:disc">큰 값이 텍스트 임베딩 안에 많이 포함되어 있음.</li></ul><ul id="1e1451cf-7b79-8026-9614-def1962fcd89" class="bulleted-list"><li style="list-style-type:disc">Cross-Attention 연산 중 수치 폭발(NaN)로 이어짐.</li></ul><p id="1e1451cf-7b79-8031-b690-d300317ceda6" class="">
</p><p id="1e1451cf-7b79-804e-aef8-fdbb3efc4073" class="">
</p><p id="1e1451cf-7b79-8001-8fd1-f5cc9669c842" class=""><strong>방법 1: RMSNorm 추가</strong></p><p id="1e1451cf-7b79-80b4-98ad-fe26f1b6d0ac" class="">Gemma-2의 텍스트 임베딩 출력에 <strong>RMSNorm</strong>을 적용</p><p id="1e1451cf-7b79-8078-9787-eafb501d7fb3" class="">RMSNorm?</p><ul id="1e1451cf-7b79-8012-a820-e765c9344d3f" class="bulleted-list"><li style="list-style-type:disc">입력 벡터의 Variance를 1.0으로 정규화</li></ul><ul id="1e1451cf-7b79-8076-ae1f-e8a87434b2f8" class="bulleted-list"><li style="list-style-type:disc">큰 값이나 작은 값들을 균일하게 만들어 수치 폭발 방지</li></ul><p id="1e1451cf-7b79-80f0-b60a-c9e3b3af8af5" class="">
</p><p id="1e1451cf-7b79-8048-8d2e-c64c9ae9f503" class=""><strong>방법 2: Learnable Scale Factor 추가</strong></p><ul id="1e1451cf-7b79-809e-9925-db946a6f80b3" class="bulleted-list"><li style="list-style-type:disc">추가로, 텍스트 임베딩에 <strong>학습 가능한 작은 스케일 파라미터</strong>를 곱함</li></ul><ul id="1e1451cf-7b79-806d-8abd-e1ae93b12c85" class="bulleted-list"><li style="list-style-type:disc">초기 값은 매우 작게 설정함 (예: 0.01)</li></ul><ul id="1e1451cf-7b79-80bd-b0df-d8d9be4e1760" class="bulleted-list"><li style="list-style-type:disc">이 파라미터가 학습을 통해 적절한 크기로 조정되면서 모델 수렴 속도가 빨라짐</li></ul><p id="1e1451cf-7b79-802e-a6b5-c1b025bb65b8" class="">
</p><p id="1e1451cf-7b79-804b-b578-d6cc57be261c" class="">→ 훈련 안정성 확보 + 수렴 속도 향상</p><figure id="1e1451cf-7b79-806c-bab4-dbe22b1d2d8a" class="image"><a href="/files/2025-05-12-SANA/image%2013.png"><img style="width:432px" src="/files/2025-05-12-SANA/image%2013.png"/></a></figure><p id="1e1451cf-7b79-80fa-b5f7-faf61ae8ad26" class="">
</p><p id="1e1451cf-7b79-80cf-a529-de6e32ac4680" class="">
</p><h3 id="1e1451cf-7b79-80cb-a7ce-cc817e85d83c" class="">Complex Human Instruction Improves Text-Image Alignment</h3><p id="1e1451cf-7b79-80cc-a519-d914d84ec1e2" class="">Gemma는 강력한 LLM이지만, 사용자가 짧거나 모호한 프롬프트를 입력하면 (예: &quot;a cat&quot;)</p><p id="1e1451cf-7b79-808c-9e94-d9334e39607c" class="">LLM이 초점을 잃고 엉뚱한 답변을 할 수도 있음.</p><p id="1e1451cf-7b79-803f-bd4d-f77b3a1d6d15" class="">→ LLM이 프롬프트에만 집중하게 만드는 추가 지시문이 필요함.</p><p id="1e1451cf-7b79-8024-8c55-e16bff64d2de" class="">
</p><p id="1e1451cf-7b79-80df-a6a3-cbe872cf84aa" class=""><strong>CHI가 그래서 뭔디?</strong></p><p id="1e1451cf-7b79-8099-bd13-cfd1ca105562" class="">LLM의 <strong>In-Context Learning</strong> 능력을 활용하여 프롬프트를 주기 전에, </p><p id="1e1451cf-7b79-80cd-9d6b-d850e580600a" class="">LLM에게 &quot;색상, 크기, 위치 관계 같은 세부 묘사를 추가해라&quot;와 같은 <strong>복잡한 명령 세트</strong>를 함께 제공하는 것</p><p id="1e1451cf-7b79-80cd-a1cb-ee60c69e658e" class="">
</p><p id="1e1451cf-7b79-8040-85bf-ed8dd8f6f44a" class=""><strong>결과 1</strong></p><div id="1e1451cf-7b79-8097-9a43-cfbef9f6121b" class="column-list"><div id="1e1451cf-7b79-80fc-98ed-c84ac075c157" style="width:37.5%" class="column"><figure id="1e1451cf-7b79-80e0-81d6-e22d8fd2e0e3" class="image"><a href="/files/2025-05-12-SANA/image%2014.png"><img style="width:720px" src="/files/2025-05-12-SANA/image%2014.png"/></a></figure></div><div id="1e1451cf-7b79-8010-8392-f60964428dc6" style="width:62.5%" class="column"><p id="1e1451cf-7b79-80b5-93db-ed7bee07f91c" class="">CHI를 적용했을 때, 학습을 처음부터 하든(fresh training)</p><p id="1e1451cf-7b79-805d-afd8-fe29ade9f100" class="">아니면 기존 모델을 미세 조정(fine-tuning)하든</p><p id="1e1451cf-7b79-807f-bbf4-ff9345eef635" class=""><strong>텍스트-이미지 정렬 성능이 향상</strong></p></div></div><p id="1e1451cf-7b79-8062-bd1b-eaaca4714b77" class="">
</p><p id="1e1451cf-7b79-8031-a545-d1b7666be5b2" class=""><strong>결과 2</strong></p><div id="1e1451cf-7b79-80e6-aca7-dc6e49816ca7" class="column-list"><div id="1e1451cf-7b79-8051-bb91-e4e5def3544c" style="width:50%" class="column"><figure id="1e1451cf-7b79-8024-8875-f8fdfcbb933a" class="image"><a href="/files/2025-05-12-SANA/image%2015.png"><img style="width:331.9921875px" src="/files/2025-05-12-SANA/image%2015.png"/></a></figure></div><div id="1e1451cf-7b79-8075-91ef-fee18cd122f8" style="width:50%" class="column"><p id="1e1451cf-7b79-80c8-90d4-cd4bd6126a58" class="">짧은 프롬프트(예: &quot;a cat&quot;)를 입력했을 때,</p><p id="1e1451cf-7b79-80da-8423-d573e7a5d20f" class="">
</p><p id="1e1451cf-7b79-80a6-a3f1-c0989f4cb298" class="">CHI가 없으면, 모델이 엉뚱한 이미지를 생성하거나 품질이 불안정해짐.</p><p id="1e1451cf-7b79-8069-8833-eec57a78702c" class="">
</p><p id="1e1451cf-7b79-8003-83a8-fa421069e30a" class="">CHI가 있으면, 모델이 <strong>프롬프트에 정확히 맞는 안정적인 이미지</strong>를 생성</p><p id="1e1451cf-7b79-80a9-8a48-d16a12f6e9d7" class="">
</p></div></div><p id="1e1451cf-7b79-80c2-8eaa-d69f82999df9" class="">
</p><p id="1e1451cf-7b79-80cf-be9f-fd35f11fe6f8" class="">
</p><p id="1e1451cf-7b79-807d-93e8-dd3ef8b78aad" class="">
</p><p id="1e1451cf-7b79-8042-8caa-cbb23bed9adf" class="">
</p><p id="1e1451cf-7b79-80c0-b0b3-e6b1bae0e301" class="">
</p><h1 id="1e1451cf-7b79-800b-9684-f23c46215e94" class="">3 EFFICIENT TRAINING/INFERENCE</h1><h2 id="1e1451cf-7b79-80c9-a061-c5455d8655d4" class="">3.1 DATA CURATION AND BLENDING</h2><h3 id="1e1451cf-7b79-80a3-8afc-e633623249c6" class="">1. Multi-Caption Auto-labelling Pipeline</h3><p id="1e1451cf-7b79-8085-a059-cdd9d0d17de5" class="">
</p><p id="1e1451cf-7b79-80ca-939b-cd45e6a4ae32" class=""><strong>이미지 하나당 4개의 VLM(Vision-Language Models) 을 이용해 캡션을 생성함.</strong></p><ul id="1e1451cf-7b79-8062-89b4-e948bb033550" class="bulleted-list"><li style="list-style-type:disc"><strong>VILA-3B</strong></li></ul><ul id="1e1451cf-7b79-80e0-b89f-f929dcf8d63e" class="bulleted-list"><li style="list-style-type:disc"><strong>VILA-13B</strong></li></ul><ul id="1e1451cf-7b79-8023-b53a-ceeac0d89062" class="bulleted-list"><li style="list-style-type:disc"><strong>InternVL-28B</strong></li></ul><ul id="1e1451cf-7b79-8091-be5e-c960755a85f0" class="bulleted-list"><li style="list-style-type:disc"><strong>InternVL-26B</strong></li></ul><p id="1e1451cf-7b79-80b8-85c7-c591d1067a22" class="">
</p><p id="1e1451cf-7b79-804f-9bbe-c8bcb1f2c8e8" class=""><strong>→ 정확한 캡션</strong> 생성 (하나만 쓰는 것보다 오류 줄임)</p><p id="1e1451cf-7b79-8027-8b9b-e6d1373614f5" class=""><strong>→ 다양한 표현</strong> 확보 (같은 이미지를 여러 관점에서 묘사 가능)</p><hr id="1e1451cf-7b79-8045-bc0d-c14849aa4cea"/><h3 id="1e1451cf-7b79-806c-b075-cc590274effc" class="">2. CLIP-Score-based Caption Sampler</h3><p id="1e1451cf-7b79-8063-a310-d7dfdf81a3d6" class=""><strong>문제 상황</strong></p><ul id="1e1451cf-7b79-8068-a41a-d7a2c2d50e26" class="bulleted-list"><li style="list-style-type:disc">캡션을 여러 개 만들었는데,<p id="1e1451cf-7b79-803c-9511-e555a8e2e77c" class="">훈련할 때 어떤 캡션을 선택할지가 문제임.</p></li></ul><ul id="1e1451cf-7b79-800c-a95f-d4a6c50ba213" class="bulleted-list"><li style="list-style-type:disc">무작위로(random) 하나 고르면:<ul id="1e1451cf-7b79-80bb-9c08-f1ad75cc5f38" class="bulleted-list"><li style="list-style-type:circle">품질이 낮은 문장을 뽑을 위험이 있음</li></ul><ul id="1e1451cf-7b79-8011-96df-ce2780c002d7" class="bulleted-list"><li style="list-style-type:circle">그러면 훈련이 느려지거나 모델 품질이 떨어짐</li></ul></li></ul><p id="1e1451cf-7b79-8033-994d-c2063d8231da" class=""><strong>해결 방법</strong></p><ul id="1e1451cf-7b79-80c6-8aab-e8729a934b60" class="bulleted-list"><li style="list-style-type:disc"><strong>CLIP score</strong>를 활용해 품질 높은 캡션을 뽑는 방식 사용<ul id="1e1451cf-7b79-8099-ac01-d3a9bfba0cb9" class="bulleted-list"><li style="list-style-type:circle">CLIP은 이미지-텍스트 매칭 정도를 점수로 계산해줌.</li></ul></li></ul><ul id="1e1451cf-7b79-8026-98b1-e209bd2f3278" class="bulleted-list"><li style="list-style-type:disc">과정:<ol type="1" id="1e1451cf-7b79-8043-a7e4-f032f1e74436" class="numbered-list" start="1"><li>이미지에 대해 생성된 각 캡션의 CLIP 점수(cic_ici)를 계산</li></ol><ol type="1" id="1e1451cf-7b79-8025-b743-d57f63576136" class="numbered-list" start="2"><li>점수가 높은 캡션일수록 뽑힐 확률이 높게 설정</li></ol><ol type="1" id="1e1451cf-7b79-8054-b3c3-f5be9f794345" class="numbered-list" start="3"><li>Sampling 확률 공식:</li></ol></li></ul><figure id="1e1451cf-7b79-809c-99ff-c50a8363d55a" class="image"><a href="/files/2025-05-12-SANA/image%2016.png"><img style="width:288px" src="/files/2025-05-12-SANA/image%2016.png"/></a></figure><p id="1e1451cf-7b79-80aa-b764-e9b1e0adae90" class="">여기</p><ul id="1e1451cf-7b79-8047-b561-fab687ac56e7" class="bulleted-list"><li style="list-style-type:disc">τ는 &quot;temperature&quot;라는 하이퍼파라미터임.</li></ul><ul id="1e1451cf-7b79-808f-a55e-e11f19d125ca" class="bulleted-list"><li style="list-style-type:disc"><strong>Temperature 조정으로 뽑는 강도를 조절</strong>할 수 있음:<ul id="1e1451cf-7b79-80c9-97fe-fb24dcc9a0a7" class="bulleted-list"><li style="list-style-type:circle">τ가 작으면: 점수 가장 높은 캡션만 거의 항상 선택</li></ul><ul id="1e1451cf-7b79-80bb-99cf-f34168f406df" class="bulleted-list"><li style="list-style-type:circle">τ가 크면: 다양한 캡션이 고르게 선택됨</li></ul></li></ul><hr id="1e1451cf-7b79-808f-b710-c6a09165c830"/><h3 id="1e1451cf-7b79-8032-a178-e8ad6f17eeb0" class="">실험 결과</h3><ul id="1e1451cf-7b79-80ee-b564-eeba1713eecc" class="bulleted-list"><li style="list-style-type:disc">Table 4 결과에 따르면:<ul id="1e1451cf-7b79-8096-8bc1-c68f3e949508" class="bulleted-list"><li style="list-style-type:circle">캡션을 다양하게 골라도 이미지 품질(FID)은 거의 변하지 않음</li></ul><ul id="1e1451cf-7b79-8054-9ec8-d80b4df23d74" class="bulleted-list"><li style="list-style-type:circle">하지만 <strong>훈련 중 텍스트-이미지 정렬</strong>(semantic alignment)은 훨씬 좋아짐</li></ul></li></ul><hr id="1e1451cf-7b79-80ed-9c5d-c6e329e8175d"/><h3 id="1e1451cf-7b79-804a-90b0-ea6fe1d64f93" class=""><strong>3. Cascade Resolution Training</strong></h3><h3 id="1e1451cf-7b79-803c-983b-fa1297a848b9" class="">기존 방식</h3><ul id="1e1451cf-7b79-8064-9ab0-c2358252adcf" class="bulleted-list"><li style="list-style-type:disc">대부분의 diffusion 모델은 해상도 256px짜리 이미지로 먼저 pre-training을 함.</li></ul><ul id="1e1451cf-7b79-80d6-aad7-ef76435bb2b1" class="bulleted-list"><li style="list-style-type:disc">이유는 연산 비용(cost)을 줄이기 위해서임.</li></ul><h3 id="1e1451cf-7b79-8058-8a3c-e9acdce89ac6" class="">문제점</h3><ul id="1e1451cf-7b79-8082-adfe-df93bd3290b9" class="bulleted-list"><li style="list-style-type:disc">256px 이미지는 디테일(detail) 손실이 심함.</li></ul><ul id="1e1451cf-7b79-80c1-aa27-f326cb7e4958" class="bulleted-list"><li style="list-style-type:disc">따라서, 작은 해상도에서 학습을 시작하면:<ul id="1e1451cf-7b79-80f9-b6c5-e7b3fbcaadb5" class="bulleted-list"><li style="list-style-type:circle">모델이 fine한 구조나 텍스처를 배우기 어려움</li></ul><ul id="1e1451cf-7b79-80ee-93b8-faf3a2995a0f" class="bulleted-list"><li style="list-style-type:circle">결국 큰 해상도로 갈 때 더 느리게 학습함.</li></ul></li></ul><hr id="1e1451cf-7b79-8027-b3fb-ee5d3d187009"/><h3 id="1e1451cf-7b79-800a-b78e-f05f748bbfbc" class="">SANA 방식</h3><ul id="1e1451cf-7b79-8046-b9fd-c61e3bc711a8" class="bulleted-list"><li style="list-style-type:disc">SANA는 <strong>AE-F32C32P1 구조</strong>를 사용하기 때문에<p id="1e1451cf-7b79-8073-8a98-ed24f26e3315" class="">latent 공간이 매우 작음 → 연산 부담이 적음.</p></li></ul><ul id="1e1451cf-7b79-80dd-a408-e625b5d076f0" class="bulleted-list"><li style="list-style-type:disc">그래서 굳이 256px에서 시작할 필요 없이 바로 <strong>512px</strong>에서 학습 시작함.</li></ul><ul id="1e1451cf-7b79-80bb-83e4-e5ad7ec945a0" class="bulleted-list"><li style="list-style-type:disc">학습 순서:<ul id="1e1451cf-7b79-80c7-aa2d-c8fb100911dc" class="bulleted-list"><li style="list-style-type:circle">512px → 1024px → 2K → 4K 순서로 점진적(fine-tuning)으로 해상도를 올림.</li></ul></li></ul><p id="1e1451cf-7b79-80ca-b57e-d4c35d9bcd24" class="">
</p><p id="1e6451cf-7b79-80d0-b860-dab650825d1a" class="">
</p><p id="1e3451cf-7b79-8000-adb6-fc4a81565262" class="">
</p><h2 id="1e1451cf-7b79-804f-9d03-e62702ab06fa" class="">3.2 FLOW-BASED TRAINING / INFERENCE</h2><p id="1e1451cf-7b79-80fa-82f7-f963983f45e1" class="">
</p><h3 id="1e1451cf-7b79-80ab-be37-cb2192fcc930" class="">Flow-based Training</h3><p id="1e5451cf-7b79-804f-8779-d781983a6267" class="">기존 방식 : noise prediction</p><div id="1e5451cf-7b79-8020-bf82-e45b81371a7c" class="column-list"><div id="1e5451cf-7b79-806e-b3d2-e0fd409c457c" style="width:33.333333333333336%" class="column"><figure id="1e5451cf-7b79-8027-9bc3-f99279a04efa" class="image"><a href="/files/2025-05-12-SANA/image%2017.png"><img style="width:331.984375px" src="/files/2025-05-12-SANA/image%2017.png"/></a></figure></div><div id="1e5451cf-7b79-80fc-9ca3-f97f3f654fc9" style="width:33.333333333333336%" class="column"><figure id="1e5451cf-7b79-80b7-9991-fb1220b4fc03" class="image"><a href="/files/2025-05-12-SANA/image%2018.png"><img style="width:205.9609375px" src="/files/2025-05-12-SANA/image%2018.png"/></a></figure></div><div id="1e5451cf-7b79-80d7-a0d7-d05dd107759f" style="width:33.33333333333333%" class="column"><figure id="1e5451cf-7b79-804d-98cd-d5a529c0c274" class="image"><a href="/files/2025-05-12-SANA/image%2019.png"><img style="width:205.9921875px" src="/files/2025-05-12-SANA/image%2019.png"/></a></figure></div></div><p id="1e5451cf-7b79-80b0-898e-db75c71747be" class="">→ 노이즈를 맞추는 것이 학습 목표</p><p id="1e5451cf-7b79-804c-82b1-ffb745ac961d" class="">
</p><p id="1e5451cf-7b79-80ab-9f70-c8a0439c09cb" class="">문제점 : t가 커지면 (Diffusion 마지막 단계에 가까우면) 노이즈가 커져서 예측 불안정</p><p id="1e5451cf-7b79-80cb-8a6b-df9d90b66f35" class="">
</p><p id="1e5451cf-7b79-803f-bf2e-f524126a0398" class="">새 방식 : EDM ,RF</p><p id="1e5451cf-7b79-80c6-9622-c597c20dfde8" class="">noise 대신 data나 velocity(노이즈와 원본 이미지 차이) 예측</p><figure id="1e5451cf-7b79-802b-bdde-d534a5c9450f" class="image"><a href="/files/2025-05-12-SANA/image%2020.png"><img style="width:710px" src="/files/2025-05-12-SANA/image%2020.png"/></a></figure><figure id="1e6451cf-7b79-80b7-a165-f564b937197a" class="image"><a href="/files/2025-05-12-SANA/image%2021.png"><img style="width:328px" src="/files/2025-05-12-SANA/image%2021.png"/></a></figure><figure id="1e5451cf-7b79-809f-86f2-cc41877713ad" class="image"><a href="/files/2025-05-12-SANA/image%2022.png"><img style="width:466px" src="/files/2025-05-12-SANA/image%2022.png"/></a></figure><p id="1e5451cf-7b79-80af-97b5-d73c0e96d93b" class="">결국 RF를 사용하여 cumulative(누적) error를 줄일 수 있음</p><p id="1e6451cf-7b79-807e-a4ea-f65080c9559c" class="">
</p><p id="1e6451cf-7b79-80b2-bef3-c2194b8546c9" class="">
</p><p id="1e5451cf-7b79-806c-870d-c391be81758d" class="">
</p><h3 id="1e5451cf-7b79-8036-ba46-cbbfa2cd97b2" class="">Flow-based Inference</h3><p id="1e5451cf-7b79-8017-a15d-d2b8eff3bc51" class="">기존 : DPM-Solver++</p><p id="1e6451cf-7b79-803f-b892-f7f4c4a2208b" class="">→ required 28-50 steps for high-quality samples</p><p id="1e6451cf-7b79-806e-8a45-d018707854fc" class="">
</p><p id="1e6451cf-7b79-8015-a23d-d46d8851693a" class=""> 현재 : <strong>Flow-DPM-Solver</strong></p><p id="1e6451cf-7b79-8011-88e9-d7e4bc9bfb8c" class="">1.Not predict original data, but velocity</p><p id="1e6451cf-7b79-8094-ae9d-f9050732e663" class="">2.substituting the scaling factor αt with 1 − σt</p><p id="1e6451cf-7b79-801a-8033-fde7491c65f8" class="">3.time-steps are redefined over the range [0, 1] instead of [1, 1000]</p><p id="1e6451cf-7b79-80cb-8be1-e320f8d3b905" class="">→Generate high-quality samples in 14-20 steps</p><p id="1e5451cf-7b79-806a-bd87-e21ae51b6fa9" class="">
</p><p id="1e5451cf-7b79-80e0-bd0f-f95736d6ae1c" class="">
</p><p id="1e5451cf-7b79-80a0-9e03-d6cb39962f0d" class="">
</p><h1 id="1e6451cf-7b79-80a9-9d46-f7c599357922" class="">5. Experiments</h1><h2 id="1e6451cf-7b79-8054-85ef-e51382a8fb29" class="">1. Model Details</h2><h3 id="1e6451cf-7b79-8019-aa5f-cb30db3bba2c" class="">Sana-0.6B</h3><ul id="1e6451cf-7b79-808b-afff-eb63079ea9b0" class="bulleted-list"><li style="list-style-type:disc">파라미터 수: <strong>590M</strong></li></ul><ul id="1e6451cf-7b79-8041-a813-d1a89556a651" class="bulleted-list"><li style="list-style-type:disc">구조: <strong>DiT-XL</strong> 및 <strong>PixArt-Σ</strong>와 거의 동일한 레이어 수와 채널 수 사용</li></ul><ul id="1e6451cf-7b79-8088-9ba7-ec6d7e252ee6" class="bulleted-list"><li style="list-style-type:disc">목적: 소형 모델로도 효율성과 품질을 동시에 확보</li></ul><h3 id="1e6451cf-7b79-8099-ac27-c712a80f2758" class="">Sana-1.6B</h3><ul id="1e6451cf-7b79-808f-90e4-fab4e5cb3141" class="bulleted-list"><li style="list-style-type:disc">파라미터 수: <strong>1.6B</strong></li></ul><ul id="1e6451cf-7b79-808c-989e-e89142f38f89" class="bulleted-list"><li style="list-style-type:disc">구조:<ul id="1e6451cf-7b79-80d5-a49a-e7df7c1b77b7" class="bulleted-list"><li style="list-style-type:circle"><strong>20개의 Transformer 레이어</strong></li></ul><ul id="1e6451cf-7b79-8084-84e9-e6d56759f224" class="bulleted-list"><li style="list-style-type:circle">각 레이어마다 <strong>2240개의 채널</strong></li></ul><ul id="1e6451cf-7b79-80a3-afb8-e9c0e9dc135d" class="bulleted-list"><li style="list-style-type:circle">FFN 내부 채널 수는 <strong>5600</strong></li></ul></li></ul><ul id="1e6451cf-7b79-8038-a2f0-ca2cd8588ffd" class="bulleted-list"><li style="list-style-type:disc">이 구성은 학습 효율성과 생성 품질 사이의 균형을 고려한 것임</li></ul><p id="1e6451cf-7b79-80a1-9bdc-c7840f63e9df" class="">
</p><h2 id="1e6451cf-7b79-80eb-94d0-db1f98b81984" class="">2. Evaluation Details</h2><p id="1e6451cf-7b79-8048-9a9a-cf8bd861ea15" class="">SANA는 총 <strong>5가지 대표적인 평가 지표</strong>를 사용하여 성능을 평가함.</p><table id="1e6451cf-7b79-80b0-8b7a-c4f3b4a27170" class="simple-table"><tbody><tr id="1e6451cf-7b79-8082-8e8b-ef3a667a8a5d"><td id="Mhcv" class="" style="width:266px">지표</td><td id="wmxf" class="" style="width:415px">설명</td></tr><tr id="1e6451cf-7b79-80cc-92aa-d4be06f0c402"><td id="Mhcv" class="" style="width:266px"><strong>FID (Fréchet Inception Distance)</strong></td><td id="wmxf" class="" style="width:415px">이미지 품질을 수치로 측정. 낮을수록 좋음</td></tr><tr id="1e6451cf-7b79-807b-87e1-dfe7d1e388d7"><td id="Mhcv" class="" style="width:266px"><strong>CLIP Score</strong></td><td id="wmxf" class="" style="width:415px">이미지와 텍스트 간 의미적 정렬 정도 평가. 높을수록 좋음</td></tr><tr id="1e6451cf-7b79-803d-8b45-c1d540707549"><td id="Mhcv" class="" style="width:266px"><strong>GenEval</strong> (Ghosh et al., 2024)</td><td id="wmxf" class="" style="width:415px">텍스트-이미지 정렬 평가. 총 533개의 프롬프트 사용</td></tr><tr id="1e6451cf-7b79-804e-a3dd-fb860ac5b556"><td id="Mhcv" class="" style="width:266px"><strong>DPG-Bench</strong> (Hu et al., 2024)</td><td id="wmxf" class="" style="width:415px">텍스트-이미지 정렬 정밀도 테스트. 1065개의 프롬프트 사용</td></tr><tr id="1e6451cf-7b79-80ba-861c-ec42cb02e2bf"><td id="Mhcv" class="" style="width:266px"><strong>ImageReward</strong> (Xu et al., 2024)</td><td id="wmxf" class="" style="width:415px">인간의 주관적 선호도를 반영한 점수. 100개 프롬프트로 측정</td></tr></tbody></table><h2 id="1e6451cf-7b79-8055-96ff-f1858372d0d3" class="">3. 평가 데이터셋</h2><ul id="1e6451cf-7b79-8019-9207-e3baac39b740" class="bulleted-list"><li style="list-style-type:disc"><strong>MJHQ-30K (Li et al., 2024a)</strong><ul id="1e6451cf-7b79-8003-b51c-dd5a5e658d0a" class="bulleted-list"><li style="list-style-type:circle">Midjourney에서 수집한 <strong>30,000개 고품질 이미지</strong> 포함</li></ul><ul id="1e6451cf-7b79-8087-bd99-c7658039857e" class="bulleted-list"><li style="list-style-type:circle">FID, CLIP Score 측정에 사용됨</li></ul></li></ul><p id="1e6451cf-7b79-8013-9261-c400f8d75156" class="">
</p><p id="1e6451cf-7b79-807e-bffa-c3bd8d38d021" class="">
</p><p id="1e6451cf-7b79-8073-97b9-e661905a16b8" class="">
</p><figure id="1e6451cf-7b79-80a1-ab7f-debea7e735cc" class="image"><a href="/files/2025-05-12-SANA/image%2023.png"><img style="width:2106px" src="/files/2025-05-12-SANA/image%2023.png"/></a></figure><p id="1e6451cf-7b79-803a-aabd-d0693c398975" class="">
</p><figure id="1e6451cf-7b79-8068-b7bf-e8694a7eef26" class="image"><a href="/files/2025-05-12-SANA/image%2024.png"><img style="width:2066px" src="/files/2025-05-12-SANA/image%2024.png"/></a></figure><p id="1e5451cf-7b79-80d7-b735-cf98c5cf027c" class="">
</p><h1 id="1b2451cf-7b79-80c7-9286-cdd226e72d91" class="">Limitation </h1><div id="1b4451cf-7b79-8041-9a12-e857f944912f" class="column-list"><div id="1b4451cf-7b79-8086-b547-df4cad250355" style="width:50%" class="column"><figure id="1b4451cf-7b79-80df-8ac8-e6b0c048fdef" class="image"><a href="/files/2025-05-12-SANA/image%2025.png"><img style="width:384px" src="/files/2025-05-12-SANA/image%2025.png"/></a></figure></div><div id="1b4451cf-7b79-8029-a1b0-e5670ac40c91" style="width:50%" class="column"><p id="1b4451cf-7b79-8091-be4f-e77e937baefd" class=""><a href="https://x.com/cloneofsimo/status/1864309440356470894?s=46">https://x.com/cloneofsimo/status/1864309440356470894?s=46</a></p><p id="1b4451cf-7b79-80ea-80cf-def1f0eca2b0" class="">코드가 전반적으로 NVIDIA칩만을 위해 설계되어</p><p id="1b4451cf-7b79-80d5-86d5-e8b9b61541b8" class="">다른 GPU 장비는 물론이고,</p><p id="1b4451cf-7b79-8017-b4b9-f4b9b6596a9e" class="">Mobile Device에서는 당연히 불가능함.</p><p id="1e1451cf-7b79-8060-991b-d4e643bec45e" class="">
</p><p id="1e1451cf-7b79-806a-9a6e-d51a61ba56ab" class="">아무래도 NVIDIA에서 낸 논문이기 때문에 Blackwell chip 홍보 겸 NVIDIA chip에서만 가능하도록 한듯.</p><p id="1b4451cf-7b79-8027-a7db-c8f0b55f4d83" class="">
</p></div></div><p id="1e6451cf-7b79-80a2-b6c4-c6e8aa0fa870" class="">
</p><p id="1e6451cf-7b79-80ea-aa53-c9c054e1998c" class="">
</p></div>