---
layout: blog
title: "[논문리뷰] SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models"
subtitle: ""
date: 2025-03-13 14:20:00 +09:00
categories: Paper
author: "hoonably"
# meta: "Springfield"
---

<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	/* white-space: pre-wrap; */
}

/* a, */
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}



.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}



.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}


.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1ae451cf-7b79-80db-bb50-e1040bab6229" class="page sans"><header><p class="page-description"></p><table class="properties"><tbody><tr class="property-row property-row-text"><th><span class="icon property-icon"><svg aria-hidden="true" role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesText"><path d="M1.56738 3.25879H14.4258C14.7676 3.25879 15.0479 2.97852 15.0479 2.63672C15.0479 2.29492 14.7744 2.02148 14.4258 2.02148H1.56738C1.21875 2.02148 0.952148 2.29492 0.952148 2.63672C0.952148 2.97852 1.22559 3.25879 1.56738 3.25879ZM1.56738 6.84082H14.4258C14.7676 6.84082 15.0479 6.56055 15.0479 6.21875C15.0479 5.87695 14.7744 5.60352 14.4258 5.60352H1.56738C1.21875 5.60352 0.952148 5.87695 0.952148 6.21875C0.952148 6.56055 1.22559 6.84082 1.56738 6.84082ZM1.56738 10.4229H14.4258C14.7676 10.4229 15.0479 10.1426 15.0479 9.80078C15.0479 9.45898 14.7744 9.18555 14.4258 9.18555H1.56738C1.21875 9.18555 0.952148 9.45898 0.952148 9.80078C0.952148 10.1426 1.22559 10.4229 1.56738 10.4229ZM1.56738 14.0049H8.75879C9.10059 14.0049 9.38086 13.7246 9.38086 13.3828C9.38086 13.041 9.10742 12.7676 8.75879 12.7676H1.56738C1.21875 12.7676 0.952148 13.041 0.952148 13.3828C0.952148 13.7246 1.22559 14.0049 1.56738 14.0049Z"></path></svg></span>Authors</th><td>Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han<br/>MIT, NVIDIA, CMU, Princeton, UC Berkeley, SJTU, Pika Labs<br/></td></tr><tr class="property-row property-row-multi_select"><th><span class="icon property-icon"><svg aria-hidden="true" role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesMultipleSelect"><path d="M1.91602 4.83789C2.44238 4.83789 2.87305 4.40723 2.87305 3.87402C2.87305 3.34766 2.44238 2.91699 1.91602 2.91699C1.38281 2.91699 0.952148 3.34766 0.952148 3.87402C0.952148 4.40723 1.38281 4.83789 1.91602 4.83789ZM5.1084 4.52344H14.3984C14.7607 4.52344 15.0479 4.23633 15.0479 3.87402C15.0479 3.51172 14.7607 3.22461 14.3984 3.22461H5.1084C4.74609 3.22461 4.45898 3.51172 4.45898 3.87402C4.45898 4.23633 4.74609 4.52344 5.1084 4.52344ZM1.91602 9.03516C2.44238 9.03516 2.87305 8.60449 2.87305 8.07129C2.87305 7.54492 2.44238 7.11426 1.91602 7.11426C1.38281 7.11426 0.952148 7.54492 0.952148 8.07129C0.952148 8.60449 1.38281 9.03516 1.91602 9.03516ZM5.1084 8.7207H14.3984C14.7607 8.7207 15.0479 8.43359 15.0479 8.07129C15.0479 7.70898 14.7607 7.42188 14.3984 7.42188H5.1084C4.74609 7.42188 4.45898 7.70898 4.45898 8.07129C4.45898 8.43359 4.74609 8.7207 5.1084 8.7207ZM1.91602 13.2324C2.44238 13.2324 2.87305 12.8018 2.87305 12.2686C2.87305 11.7422 2.44238 11.3115 1.91602 11.3115C1.38281 11.3115 0.952148 11.7422 0.952148 12.2686C0.952148 12.8018 1.38281 13.2324 1.91602 13.2324ZM5.1084 12.918H14.3984C14.7607 12.918 15.0479 12.6309 15.0479 12.2686C15.0479 11.9062 14.7607 11.6191 14.3984 11.6191H5.1084C4.74609 11.6191 4.45898 11.9062 4.45898 12.2686C4.45898 12.6309 4.74609 12.918 5.1084 12.918Z"></path></svg></span>Venue &amp; Year</th><td><span class="selected-value select-value-color-default">25</span><span class="selected-value select-value-color-red">ICLR</span><span class="selected-value select-value-color-green">Spotlight</span></td></tr><tr class="property-row property-row-date"><th><span class="icon property-icon"><svg aria-hidden="true" role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesDate"><path d="M3.29688 14.4561H12.7031C14.1797 14.4561 14.9453 13.6904 14.9453 12.2344V3.91504C14.9453 2.45215 14.1797 1.69336 12.7031 1.69336H3.29688C1.82031 1.69336 1.05469 2.45215 1.05469 3.91504V12.2344C1.05469 13.6973 1.82031 14.4561 3.29688 14.4561ZM3.27637 13.1162C2.70898 13.1162 2.39453 12.8154 2.39453 12.2207V5.9043C2.39453 5.30273 2.70898 5.00879 3.27637 5.00879H12.71C13.2842 5.00879 13.6055 5.30273 13.6055 5.9043V12.2207C13.6055 12.8154 13.2842 13.1162 12.71 13.1162H3.27637ZM6.68066 7.38086H7.08398C7.33008 7.38086 7.41211 7.30566 7.41211 7.05957V6.66309C7.41211 6.41699 7.33008 6.3418 7.08398 6.3418H6.68066C6.44141 6.3418 6.35938 6.41699 6.35938 6.66309V7.05957C6.35938 7.30566 6.44141 7.38086 6.68066 7.38086ZM8.92285 7.38086H9.31934C9.56543 7.38086 9.64746 7.30566 9.64746 7.05957V6.66309C9.64746 6.41699 9.56543 6.3418 9.31934 6.3418H8.92285C8.67676 6.3418 8.59473 6.41699 8.59473 6.66309V7.05957C8.59473 7.30566 8.67676 7.38086 8.92285 7.38086ZM11.1582 7.38086H11.5547C11.8008 7.38086 11.8828 7.30566 11.8828 7.05957V6.66309C11.8828 6.41699 11.8008 6.3418 11.5547 6.3418H11.1582C10.9121 6.3418 10.8301 6.41699 10.8301 6.66309V7.05957C10.8301 7.30566 10.9121 7.38086 11.1582 7.38086ZM4.44531 9.58203H4.84863C5.09473 9.58203 5.17676 9.50684 5.17676 9.26074V8.86426C5.17676 8.61816 5.09473 8.54297 4.84863 8.54297H4.44531C4.20605 8.54297 4.12402 8.61816 4.12402 8.86426V9.26074C4.12402 9.50684 4.20605 9.58203 4.44531 9.58203ZM6.68066 9.58203H7.08398C7.33008 9.58203 7.41211 9.50684 7.41211 9.26074V8.86426C7.41211 8.61816 7.33008 8.54297 7.08398 8.54297H6.68066C6.44141 8.54297 6.35938 8.61816 6.35938 8.86426V9.26074C6.35938 9.50684 6.44141 9.58203 6.68066 9.58203ZM8.92285 9.58203H9.31934C9.56543 9.58203 9.64746 9.50684 9.64746 9.26074V8.86426C9.64746 8.61816 9.56543 8.54297 9.31934 8.54297H8.92285C8.67676 8.54297 8.59473 8.61816 8.59473 8.86426V9.26074C8.59473 9.50684 8.67676 9.58203 8.92285 9.58203ZM11.1582 9.58203H11.5547C11.8008 9.58203 11.8828 9.50684 11.8828 9.26074V8.86426C11.8828 8.61816 11.8008 8.54297 11.5547 8.54297H11.1582C10.9121 8.54297 10.8301 8.61816 10.8301 8.86426V9.26074C10.8301 9.50684 10.9121 9.58203 11.1582 9.58203ZM4.44531 11.7832H4.84863C5.09473 11.7832 5.17676 11.708 5.17676 11.4619V11.0654C5.17676 10.8193 5.09473 10.7441 4.84863 10.7441H4.44531C4.20605 10.7441 4.12402 10.8193 4.12402 11.0654V11.4619C4.12402 11.708 4.20605 11.7832 4.44531 11.7832ZM6.68066 11.7832H7.08398C7.33008 11.7832 7.41211 11.708 7.41211 11.4619V11.0654C7.41211 10.8193 7.33008 10.7441 7.08398 10.7441H6.68066C6.44141 10.7441 6.35938 10.8193 6.35938 11.0654V11.4619C6.35938 11.708 6.44141 11.7832 6.68066 11.7832ZM8.92285 11.7832H9.31934C9.56543 11.7832 9.64746 11.708 9.64746 11.4619V11.0654C9.64746 10.8193 9.56543 10.7441 9.31934 10.7441H8.92285C8.67676 10.7441 8.59473 10.8193 8.59473 11.0654V11.4619C8.59473 11.708 8.67676 11.7832 8.92285 11.7832Z"></path></svg></span>날짜</th><td><time>@2025년 3월 6일</time></td></tr><tr class="property-row property-row-status"><th><span class="icon property-icon"><svg aria-hidden="true" role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesStatus"><path d="M8.75488 1.02344C8.75488 0.613281 8.41309 0.264648 8.00293 0.264648C7.59277 0.264648 7.25098 0.613281 7.25098 1.02344V3.11523C7.25098 3.51855 7.59277 3.86719 8.00293 3.86719C8.41309 3.86719 8.75488 3.51855 8.75488 3.11523V1.02344ZM3.91504 5.0293C4.20215 5.31641 4.69434 5.32324 4.97461 5.03613C5.26855 4.74902 5.26855 4.25684 4.98145 3.96973L3.53906 2.52051C3.25195 2.2334 2.7666 2.21973 2.47949 2.50684C2.19238 2.79395 2.18555 3.28613 2.47266 3.57324L3.91504 5.0293ZM10.9629 4.01758C10.6826 4.30469 10.6826 4.79688 10.9697 5.08398C11.2568 5.37109 11.749 5.36426 12.0361 5.07715L13.4854 3.62793C13.7725 3.34082 13.7725 2.84863 13.4785 2.55469C13.1982 2.27441 12.7061 2.27441 12.4189 2.56152L10.9629 4.01758ZM15.0234 8.78906C15.4336 8.78906 15.7822 8.44727 15.7822 8.03711C15.7822 7.62695 15.4336 7.28516 15.0234 7.28516H12.9385C12.5283 7.28516 12.1797 7.62695 12.1797 8.03711C12.1797 8.44727 12.5283 8.78906 12.9385 8.78906H15.0234ZM0.975586 7.28516C0.56543 7.28516 0.223633 7.62695 0.223633 8.03711C0.223633 8.44727 0.56543 8.78906 0.975586 8.78906H3.07422C3.48438 8.78906 3.83301 8.44727 3.83301 8.03711C3.83301 7.62695 3.48438 7.28516 3.07422 7.28516H0.975586ZM12.0361 10.9902C11.749 10.71 11.2568 10.71 10.9629 10.9971C10.6826 11.2842 10.6826 11.7764 10.9697 12.0635L12.4258 13.5127C12.7129 13.7998 13.2051 13.793 13.4922 13.5059C13.7793 13.2256 13.7725 12.7266 13.4854 12.4395L12.0361 10.9902ZM2.52051 12.4395C2.22656 12.7266 2.22656 13.2188 2.50684 13.5059C2.79395 13.793 3.28613 13.7998 3.57324 13.5127L5.02246 12.0703C5.31641 11.7832 5.31641 11.291 5.03613 11.0039C4.74902 10.7168 4.25684 10.71 3.96973 10.9971L2.52051 12.4395ZM8.75488 12.9658C8.75488 12.5557 8.41309 12.207 8.00293 12.207C7.59277 12.207 7.25098 12.5557 7.25098 12.9658V15.0576C7.25098 15.4609 7.59277 15.8096 8.00293 15.8096C8.41309 15.8096 8.75488 15.4609 8.75488 15.0576V12.9658Z"></path></svg></span>상태</th><td><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>완료</span></td></tr></tbody></table></header><div class="page-body"><table id="1ae451cf-7b79-805c-93cd-e196dc0bdeb9" class="simple-table"><tbody><tr id="1ae451cf-7b79-808c-943f-eac9eb5f2eba"><th id="MApI" class="simple-table-header-color simple-table-header" style="width:125.5px">ArXiv</th><td id="L|H:" class="" style="width:580.5px"><a href="https://arxiv.org/abs/2411.05007">https://arxiv.org/abs/2411.05007</a></td></tr><tr id="1ae451cf-7b79-8003-aee0-df288cea6508"><th id="MApI" class="simple-table-header-color simple-table-header" style="width:125.5px">Project Page</th><td id="L|H:" class="" style="width:580.5px"><a href="https://hanlab.mit.edu/projects/svdquant">https://hanlab.mit.edu/projects/svdquant</a></td></tr><tr id="1ae451cf-7b79-80e1-8c89-e694ec013045"><th id="MApI" class="simple-table-header-color simple-table-header" style="width:125.5px">Github Code</th><td id="L|H:" class="" style="width:580.5px"><a href="https://github.com/mit-han-lab/nunchaku">https://github.com/mit-han-lab/nunchaku</a></td></tr><tr id="1af451cf-7b79-800d-bf70-cf7f29ae2f57"><th id="MApI" class="simple-table-header-color simple-table-header" style="width:125.5px">Demo</th><td id="L|H:" class="" style="width:580.5px"><a href="https://svdquant.mit.edu/">https://svdquant.mit.edu/</a></td></tr></tbody></table><figure id="1b7451cf-7b79-8045-a10a-c27bbcdf50fd"><div class="source"><a href="/images/2025-03-13-SVDQuant/250313_JeonghoonPark_SVDQuant_Absorbing_Outliers_by.pptx">250313_JeonghoonPark_SVDQuant_Absorbing_Outliers_by.pptx</a></div></figure><figure id="1b7451cf-7b79-804e-888f-fff3fa53d02e"><div class="source"><a href="/images/2025-03-13-SVDQuant/250313_JeonghoonPark_SVDQuant_Absorbing_Outliers_by.pdf">250313_JeonghoonPark_SVDQuant_Absorbing_Outliers_by.pdf</a></div></figure><figure class="block-color-teal_background callout" style="white-space:pre-wrap;display:flex" id="1ae451cf-7b79-80b4-b7a9-d4f362a314d8"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="1ae451cf-7b79-800e-b074-e362d735ca42" class="">Key Differentiator</p><p id="1b4451cf-7b79-805e-ae1b-ec95b9e0e614" class="">“Outlier Absorption Using Singular Value Decomposition”</p><figure id="1b4451cf-7b79-80d5-8378-e104c018ad62" class="image"><a href="/images/2025-03-13-SVDQuant/image.png"><img style="width:643.984375px" src="/images/2025-03-13-SVDQuant/image.png"/></a></figure></div></figure><p id="1b9451cf-7b79-8032-b7ef-e5cec973616b" class="">
</p><p id="1b9451cf-7b79-8094-97e0-e2c246b2c531" class="">
</p><h1 id="1b9451cf-7b79-8012-b7d6-e14efa74183b" class="">Song Han?</h1><blockquote id="1b9451cf-7b79-80aa-8170-e45a21f2c364" class="">Song Han is an associate professor at MIT EECS. He earned his PhD from Stanford, pioneering efficient AI computing techniques such as “Deep Compression” (pruning, quantization) and the “Efficient Inference Engine,” which first introduced weight sparsity to modern AI chips, making it one of the top-5 most cited papers in the 50-year history of ISCA (1953-2023). His innovations, including TinyML and hardware-aware neural architecture search (Once-for-All Network), have advanced AI model deployment on resource-constrained devices.</blockquote><figure id="1b9451cf-7b79-80d3-a2d4-c3a2854934a1" class="image"><a href="/images/2025-03-13-SVDQuant/image%201.png"><img style="width:576px" src="/images/2025-03-13-SVDQuant/image%201.png"/></a></figure><p id="1b9451cf-7b79-80e5-8edc-f584a3a713c0" class="">
</p><p id="1b9451cf-7b79-80f8-8222-ce544cfed97c" class="">
</p><p id="1b9451cf-7b79-8021-b819-f144582b675e" class="">
</p><p id="1b9451cf-7b79-8088-bc0d-d678ad8a06cd" class="">
</p><p id="1b9451cf-7b79-8053-b1db-cd0ed566f6a7" class="">
</p><p id="1b9451cf-7b79-80a4-8ce2-d75baf6f5327" class="">
</p><p id="1af451cf-7b79-8064-a8a6-d81951bce9ee" class="">
</p><h1 id="1af451cf-7b79-8080-9bf7-e6ca0ea8ac06" class="">1. Introduction</h1><div id="1b0451cf-7b79-80cc-89bb-c5061bd83aa2" class="column-list"><div id="1b0451cf-7b79-803c-ad51-d3337be6cf87" style="width:37.5%" class="column"><figure id="1af451cf-7b79-80b9-b6cc-ef3c8c412aa2" class="image"><a href="/images/2025-03-13-SVDQuant/image%202.png"><img style="width:336px" src="/images/2025-03-13-SVDQuant/image%202.png"/></a></figure></div><div id="1b0451cf-7b79-8014-bc51-ce5bd0a7be1d" style="width:62.5%" class="column"><p id="1ae451cf-7b79-809f-a365-e27629e5708d" class="">LLM과 비교했을 때, 모델 사이즈에 따라 계산 비용이 빠르게 증가한다.</p><p id="1af451cf-7b79-80d2-a35e-da017eca03f3" class="">Moore’s law가 slow down 함으로서, 저렴한 추론(low-precision inference) 으로 전환하는중 </p><p id="1af451cf-7b79-8098-80f9-eb7a6d6223db" class="">→ 4bit floating point (FP4)가 대세임</p><p id="1b0451cf-7b79-8042-aca3-ebbe11f66404" class="">
</p></div></div><div id="1b0451cf-7b79-8032-a22b-d1683814c2c5" class="column-list"><div id="1b0451cf-7b79-80bc-8221-c4aeb1973a4e" style="width:25%" class="column"><figure id="1af451cf-7b79-80ac-b07c-ef4740e29565" class="image"><a href="/images/2025-03-13-SVDQuant/image%203.png"><img style="width:144px" src="/images/2025-03-13-SVDQuant/image%203.png"/></a></figure><p id="1b0451cf-7b79-8053-bbcc-d14dba1639fd" class="">
</p></div><div id="1b0451cf-7b79-80f9-82f2-f542e8f4941e" style="width:75%" class="column"><p id="1b0451cf-7b79-80eb-9f9b-d476809a077e" class="">LLM</p><p id="1b0451cf-7b79-80fb-bb0b-c83f6b66c59f" class="">latency는 주로 가중치(weight) 로딩 속도에 의해 결정</p><p id="1b0451cf-7b79-8082-8d58-e14653f5a1e3" class="">&quot;가중치만 양자화(weight-only quantization)&quot; 해도 속도를 개선</p><p id="1b0451cf-7b79-8099-8d68-eda637d51f0a" class="">
</p><p id="1b0451cf-7b79-806c-a2dc-c0c397f6c725" class="">Diffusion 모델</p><p id="1b0451cf-7b79-80a7-92ee-d72568410dc9" class="">레이턴시는 가중치를 불러오는 속도가 아니라, 연산량 자체가 병목</p><p id="1b0451cf-7b79-8049-9f28-d3c199676790" class="">왜냐하면 가중치만 4비트로 줄여도 활성화값이 16비트이면, 연산 과정에서 16비트로 다시 변환(upcast)되므로 연산량이 줄어들지 않음.</p><p id="1b0451cf-7b79-80fe-b855-e236e83dd90e" class="">결국 연산량을 줄이려면 가중치(weight)뿐만 아니라 활성화값(activation)도 함께 4비트로 양자화해야 함.</p></div></div><figure id="1b0451cf-7b79-8094-8134-c9ea7d1ff191" class="image"><a href="/images/2025-03-13-SVDQuant/image%204.png"><img style="width:709.984375px" src="/images/2025-03-13-SVDQuant/image%204.png"/></a></figure><p id="1b3451cf-7b79-8020-98fd-e38dfe0b7963" class="">
</p><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1b4451cf-7b79-80d5-8357-e37ef77f7fa0"><div style="font-size:1.5em"><span class="icon">📢</span></div><div style="width:100%"><ul id="1b4451cf-7b79-80a2-8541-c4c2a915f65d" class="bulleted-list"><li style="list-style-type:disc">Input Channel → 원래 Activation에서 나온 입력 채널</li></ul><ul id="1b4451cf-7b79-80bb-a2dd-f84a592890dc" class="bulleted-list"><li style="list-style-type:disc">Channel → Weight의 각 채널</li></ul></div></figure><p id="1b4451cf-7b79-8020-a22d-eda7068c31d3" class="">
</p><h3 id="1b0451cf-7b79-8031-b656-c73c4e468076" class="">1. 기존 4비트 양자화(4-bit Quantization)의 문제점</h3><ul id="1b0451cf-7b79-8023-b775-d40d640365a0" class="bulleted-list"><li style="list-style-type:disc">가중치(Weight)와 활성화값(Activation) 모두 4비트로 줄이면 품질이 크게 저하될 가능성이 높음.</li></ul><ul id="1b0451cf-7b79-80a7-87c2-ee25491cce41" class="bulleted-list"><li style="list-style-type:disc">특히 기존 방법(예: Smoothing)은 가중치와 활성화값 사이에서 Outlier를 이동시키는 방식을 사용했지만,Diffusion 모델에서는 Outlier가 양쪽(W, X) 모두에서 심각하게 발생하므로 효과적이지 않음.<ul id="1b0451cf-7b79-80be-afba-fbfd0a1bd5b0" class="bulleted-list"><li style="list-style-type:circle">기존 방식은 활성화값(X)에서 Outlier를 제거하려고 하면 가중치(W)로 이동하고, 반대로 하면 X에 Outlier가 남는 문제 발생.</li></ul></li></ul><hr id="1b0451cf-7b79-805b-82c0-fa3c3c42a9a6"/><h3 id="1b0451cf-7b79-80d7-8c42-d6af561178a5" class="">2. SVDQuant의 핵심 아이디어</h3><p id="1b0451cf-7b79-800e-9933-ceff2d93b1b1" class="">✅ Outlier를 단순히 이동하는 것이 아니라, &quot;흡수&quot;하는 방법을 사용함.</p><p id="1b0451cf-7b79-802a-ab80-e4bebc045ec2" class="">✅ 저비용의 &quot;Low-Rank Branch&quot;를 추가하여 Outlier를 가중치(W)에서 흡수함.</p><p id="1b0451cf-7b79-8008-a5f9-e816793573b0" class="">✅ 이를 위해 SVD(Singular Value Decomposition, 특이값 분해) 기법을 활용하여 가중치를 두 개의 성분으로 분해함.</p><hr id="1b0451cf-7b79-8051-961d-cfd87abd2a58"/><h3 id="1b0451cf-7b79-80fd-8208-c69bf137b1d1" class="">3. SVDQuant의 단계별 동작 방식</h3><p id="1b0451cf-7b79-8066-829c-c3df956b74ae" class="">1️⃣ Outlier 이동 (Smoothing)</p><ul id="1b0451cf-7b79-8098-8dee-fe6740ba74e5" class="bulleted-list"><li style="list-style-type:disc">먼저 Outlier를 활성화값(X)에서 가중치(W)로 이동함.</li></ul><ul id="1b0451cf-7b79-8034-b2dc-fbbdd11c23c5" class="bulleted-list"><li style="list-style-type:disc">이를 통해 활성화값(X)이 더 균일해져서 4비트 양자화가 더 쉬워짐.</li></ul><p id="1b0451cf-7b79-8029-8c6a-fada4f9ac553" class="">2️⃣ SVD(특이값 분해)를 적용하여 가중치(W)를 두 개의 성분으로 분해</p><ul id="1b0451cf-7b79-80e8-9f2d-d20f2a41cf00" class="bulleted-list"><li style="list-style-type:disc">W → L1L2(저순위 성분) + 잔여 성분(W - L1L2)로 분리</li></ul><ul id="1b0451cf-7b79-80ad-9a3d-d57977621d06" class="bulleted-list"><li style="list-style-type:disc">L1L2(저순위 성분)은 16비트로 유지하고, W - L1L2(잔여 성분)만 4비트로 양자화</li></ul><ul id="1b0451cf-7b79-800f-b3fd-e2f48cc14aba" class="bulleted-list"><li style="list-style-type:disc">즉, 저순위 성분(Low-Rank Component)이 Outlier를 흡수하면서 4비트 양자화가 더 쉬워짐.</li></ul><p id="1b0451cf-7b79-801a-998e-eac0ba146fa9" class="">3️⃣ 저순위 성분을 따로 계산하면 메모리 액세스 오버헤드가 증가하는 문제 발생</p><ul id="1b0451cf-7b79-806f-b26a-e9f0e6543328" class="bulleted-list"><li style="list-style-type:disc">즉, L1L2를 별도로 처리하면 연산 속도가 느려지는 문제가 생김.</li></ul><ul id="1b0451cf-7b79-80e3-8321-fb0b729e7c31" class="bulleted-list"><li style="list-style-type:disc">기본적으로 4비트 연산의 속도를 높이려고 했는데, 저순위 연산이 추가되면 오히려 느려질 수 있음.</li></ul><p id="1b0451cf-7b79-80af-be9b-cee524284df7" class="">4️⃣ 이를 해결하기 위해 전용 추론 엔진(Nunchaku) 설계</p><ul id="1b0451cf-7b79-80b7-b671-dd24b7a6b1cf" class="bulleted-list"><li style="list-style-type:disc">Nunchaku 엔진은 4비트 양자화 연산과 저순위 연산을 함께 최적화하여 오버헤드를 줄임.</li></ul><ul id="1b0451cf-7b79-8000-bad7-c9a13f8d770b" class="bulleted-list"><li style="list-style-type:disc">즉, L1L2(저순위 연산)와 4비트 연산을 함께 처리하는 커널(fusion kernel)로 변환하여 성능을 최적화.</li></ul><ul id="1b0451cf-7b79-80da-9587-f7e990c22ed9" class="bulleted-list"><li style="list-style-type:disc">이를 통해 추가적인 연산량이 생기더라도 실제로는 4비트 연산의 속도를 향상할 수 있도록 설계됨.</li></ul><p id="1b0451cf-7b79-8027-8d85-d72e2a8500c4" class="">
</p><p id="1b0451cf-7b79-8040-b088-e9573a3875ea" class="">
</p><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">기존 방식(SmoothQuant, AWQ)과 SVDQuant의 차이</summary><div class="indented"><table id="1b0451cf-7b79-80e9-a7be-e57d2b8883d8" class="simple-table"><tbody><tr id="1b0451cf-7b79-8035-9773-e1ac9754aafa"><td id=":Vib" class="">방법</td><td id="uAUW" class="">방식</td><td id="uBN[" class="" style="width:229.5625px">Outlier 처리 방식</td><td id="\ab\" class="">적용 대상</td><td id="d^ui" class="">문제점</td></tr><tr id="1b0451cf-7b79-80d3-b1a1-fa2a22528041"><td id=":Vib" class="">SmoothQuant (2023)</td><td id="uAUW" class="">W4A4</td><td id="uBN[" class="" style="width:229.5625px">Input Channel(Activation) → Channel(Weight)</td><td id="\ab\" class="">LLM(대형 언어 모델)</td><td id="d^ui" class="">Outlier가 가중치에 누적됨</td></tr><tr id="1b0451cf-7b79-80a7-b747-d8244d0ca7e5"><td id=":Vib" class="">AWQ (2024)</td><td id="uAUW" class="">W4A4</td><td id="uBN[" class="" style="width:229.5625px">가중치 중 중요한 부분을 보존하여 양자화</td><td id="\ab\" class="">LLM</td><td id="d^ui" class="">Diffusion 모델에서는 한계 가능성</td></tr><tr id="1b0451cf-7b79-80a2-87d0-ee757edac464"><td id=":Vib" class="">SVDQuant (2024)</td><td id="uAUW" class="">W4A4</td><td id="uBN[" class="" style="width:229.5625px">저순위(Low-Rank) 성분으로 Outlier 흡수</td><td id="\ab\" class="">Diffusion 모델 최적화</td><td id="d^ui" class="">추가 연산을 해결해야 함</td></tr></tbody></table><h3 id="1b4451cf-7b79-808d-b9f5-caf9daa665ef" class="">1. SmoothQuant (2023) – Activation에서 Weight로 이상치 이동</h3><p id="1b4451cf-7b79-809a-907c-c266d6047b9c" class="">SmoothQuant의 핵심 아이디어는 활성화값(Activation)에서 발생하는 이상치를 가중치(Weight)로 이동시키는 거야​Li 등 - 2024 - SVDQuan….</p><ul id="1b4451cf-7b79-80ce-806d-c9ecda3068f3" class="bulleted-list"><li style="list-style-type:disc">기존 문제<ul id="1b4451cf-7b79-8061-93b8-f9770e6cfaf3" class="bulleted-list"><li style="list-style-type:circle">Transformer 기반 모델에서 Self-Attention 연산이 많아서 활성화값(Activation)의 범위가 넓어지고 이상치가 발생하는 경우가 많아.</li></ul><ul id="1b4451cf-7b79-808b-b805-c308b15bd947" class="bulleted-list"><li style="list-style-type:circle">이를 8-bit이나 4-bit로 양자화하면, 작은 값들은 모두 0이 되고, 정보 손실이 심해짐.</li></ul></li></ul><ul id="1b4451cf-7b79-808b-8d2a-f4e0223d873e" class="bulleted-list"><li style="list-style-type:disc">해결 방법<ul id="1b4451cf-7b79-80b1-b04e-de65c2ea2196" class="bulleted-list"><li style="list-style-type:circle">활성화값(Activation)의 채널별 스케일링을 적용하여, 이상치를 가중치(Weight) 쪽으로 이동시킴.</li></ul><ul id="1b4451cf-7b79-80ff-9d74-ffb2864c5ba2" class="bulleted-list"><li style="list-style-type:circle">즉, 원래 Activation 값이 크면, 해당 채널을 스케일링해서 줄이고, 대신 그 값을 Weight에서 보상해주는 방식.</li></ul><ul id="1b4451cf-7b79-803f-b0f4-c4d8a873783f" class="bulleted-list"><li style="list-style-type:circle">이렇게 하면, Activation 값이 양자화할 때 손실 없이 더 균등하게 분포할 수 있음.</li></ul></li></ul><ul id="1b4451cf-7b79-80ae-ba66-e742f8eae501" class="bulleted-list"><li style="list-style-type:disc">한계<ul id="1b4451cf-7b79-80d1-89e4-c65ac1f030a5" class="bulleted-list"><li style="list-style-type:circle">Weight 쪽으로 이상치를 몰아넣으면, Weight의 값이 커지고, Weight 양자화 시 오류가 커질 가능성이 있음.</li></ul><ul id="1b4451cf-7b79-8081-93d3-ca89be233641" class="bulleted-list"><li style="list-style-type:circle">따라서 Weight를 4-bit로 양자화할 경우 정보 손실이 발생할 수 있음.</li></ul></li></ul><hr id="1b4451cf-7b79-80d4-b6ae-c77a2e75b979"/><h3 id="1b4451cf-7b79-80a1-b032-fc72c2e78684" class="">2. AWQ (Activation-aware Weight Quantization, 2024) – Weight에서 Activation으로 이상치 이동</h3><p id="1b4451cf-7b79-80a8-9004-e985dc396261" class="">AWQ는 Weight의 이상치를 줄이기 위해 Activation으로 분산시키는 방식</p><ul id="1b4451cf-7b79-80bd-ada8-cdc20583b63e" class="bulleted-list"><li style="list-style-type:disc">기존 문제<ul id="1b4451cf-7b79-804e-bc3a-c188a01173ac" class="bulleted-list"><li style="list-style-type:circle">SmoothQuant 방식처럼 이상치를 Weight 쪽으로 이동시키면, Weight의 크기가 커져서 Weight를 4-bit로 양자화할 때 정보 손실이 발생할 가능성이 높아짐.</li></ul><ul id="1b4451cf-7b79-8023-903c-dceea34e4ec4" class="bulleted-list"><li style="list-style-type:circle">특히, Weight에 이상치가 많으면, 스케일링을 적용해도 양자화 오류가 커지고 성능이 떨어지는 문제가 발생.</li></ul></li></ul><ul id="1b4451cf-7b79-8090-8d91-e5477b6ff3e6" class="bulleted-list"><li style="list-style-type:disc">해결 방법<ul id="1b4451cf-7b79-80cd-9e1c-ddf74b1b0b0c" class="bulleted-list"><li style="list-style-type:circle">대신 Weight에서 Activation으로 일부 이상치를 이동시켜서, Weight가 양자화될 때 정보 손실을 최소화함.</li></ul><ul id="1b4451cf-7b79-80bd-91cd-d05e819740ca" class="bulleted-list"><li style="list-style-type:circle">즉, 중요한 Weight 값을 따로 보호하고, 불필요한 큰 값을 Activation 쪽으로 이동시켜서 Weight를 더 균등한 분포로 만들도록 설계.</li></ul></li></ul><ul id="1b4451cf-7b79-8061-832d-d0cc31a1e0ac" class="bulleted-list"><li style="list-style-type:disc">한계<ul id="1b4451cf-7b79-80be-9f74-d007d93a0e3b" class="bulleted-list"><li style="list-style-type:circle">Activation의 분포가 다시 넓어질 가능성이 있음 → Activation을 다시 4-bit로 양자화할 경우 문제가 발생할 수도 있음.</li></ul></li></ul><hr id="1b4451cf-7b79-8002-96d5-f0c26ed3dad0"/><h3 id="1b4451cf-7b79-8002-a31f-fdb97d4588e9" class="">3. SVDQuant (2024) – Outlier를 Low-Rank Component로 이동</h3><p id="1b4451cf-7b79-80a5-b07b-d4011ec79379" class="">SVDQuant는 SmoothQuant와 AWQ의 문제점을 모두 해결하려고, 이상치를 이동시키는 것뿐만 아니라 Low-Rank Component로 흡수하는 방식이야​Li 등 - 2024 - SVDQuan….</p><ul id="1b4451cf-7b79-807f-82f9-c8a3269df2f2" class="bulleted-list"><li style="list-style-type:disc">핵심 아이디어<ul id="1b4451cf-7b79-8001-b141-fe1d31d18503" class="bulleted-list"><li style="list-style-type:circle">SmoothQuant처럼 Activation의 이상치를 Weight로 이동하면서도,</li></ul><ul id="1b4451cf-7b79-807f-89be-f56ed8145b38" class="bulleted-list"><li style="list-style-type:circle">AWQ처럼 Weight에서 다시 Activation으로 이동하는 대신, Low-Rank Component로 분리하여 저장.</li></ul><ul id="1b4451cf-7b79-80c3-9eca-f8a91c74e236" class="bulleted-list"><li style="list-style-type:circle">즉, 이상치를 양자화하지 않고, 16-bit Low-Rank Component로 유지하여 정보 손실을 최소화.</li></ul></li></ul><ul id="1b4451cf-7b79-807e-9946-c1a25580c8e5" class="bulleted-list"><li style="list-style-type:disc">장점<ul id="1b4451cf-7b79-80d6-bcac-d917aaf09579" class="bulleted-list"><li style="list-style-type:circle">SmoothQuant나 AWQ처럼 한쪽으로 이상치를 몰아넣지 않고, Low-Rank Branch가 이상치를 흡수해서 손실을 막음.</li></ul><ul id="1b4451cf-7b79-804a-b08a-fb324a788156" class="bulleted-list"><li style="list-style-type:circle">Weight와 Activation 모두 균등한 분포를 가지게 되어, 양자화 오류가 줄어듦.</li></ul><ul id="1b4451cf-7b79-80a5-b4ec-dfa95325b928" class="bulleted-list"><li style="list-style-type:circle">실제 실험에서도 SmoothQuant, AWQ보다 4-bit 양자화에서 성능이 뛰어남.</li></ul></li></ul><hr id="1b4451cf-7b79-809c-b6c1-e62cc01ddd97"/><h3 id="1b4451cf-7b79-8021-8426-fe5be52ee107" class="">결론</h3><ul id="1b4451cf-7b79-806f-ae5f-c169e9987b6c" class="bulleted-list"><li style="list-style-type:disc">SmoothQuant → Activation의 이상치를 Weight로 이동 (Weight의 정보 손실 가능성 있음)</li></ul><ul id="1b4451cf-7b79-80e6-9557-f0b75a091f7e" class="bulleted-list"><li style="list-style-type:disc">AWQ → Weight의 이상치를 Activation으로 이동 (Activation의 정보 손실 가능성 있음)</li></ul><ul id="1b4451cf-7b79-805a-85d9-dc3765f7950b" class="bulleted-list"><li style="list-style-type:disc">SVDQuant → Weight와 Activation에서 Low-Rank Component로 이동 (이상치 자체를 제거하여 정보 손실을 최소화)</li></ul><p id="1b4451cf-7b79-8047-a453-cf8f26792a35" class="">즉, SmoothQuant과 AWQ는 둘 중 하나만 Outlier를 발생하지 않도록 하려고 했던 접근법, 반면 SVDQuant는 Outlier 자체를 Low-Rank로 빼버리는 방식이라 정보 손실이 가장 적음.</p><p id="1b4451cf-7b79-80c1-b932-f1842c5e4c74" class="">
</p></div></details><p id="1b4451cf-7b79-8065-9f3f-f2f8ae9a15db" class="">
</p><p id="1b4451cf-7b79-803e-a0ef-fe7b52f25ec7" class="">
</p><p id="1b4451cf-7b79-80c0-b0a3-c337950d8bb4" class="">
</p><p id="1b4451cf-7b79-8041-ad53-eb95341fef1f" class="">
</p><p id="1b0451cf-7b79-80be-8bfd-f29d8f979a3a" class="">
</p><h1 id="1b0451cf-7b79-8044-8a33-e5b0a2de789b" class="">3 QUANTIZATION PRELIMINARY</h1><ul id="1b0451cf-7b79-803a-9411-c06a88960951" class="bulleted-list"><li style="list-style-type:disc">양자화(Quantization)의 기본 개념<ul id="1b0451cf-7b79-80a0-af1c-def5b99aab1e" class="bulleted-list"><li style="list-style-type:circle">딥러닝에서 양자화는 연산 속도를 높이고 메모리 사용량을 줄이는 데 사용되는 방법.</li></ul><ul id="1b0451cf-7b79-80be-bad2-c2c53450132c" class="bulleted-list"><li style="list-style-type:circle">텐서 X를 양자화하는 과정:<br/><br/><figure id="1b0451cf-7b79-80b2-af7a-e92befb117ed" class="image"><a href="/images/2025-03-13-SVDQuant/image%205.png"><img style="width:384px" src="/images/2025-03-13-SVDQuant/image%205.png"/></a></figure><ul id="1b0451cf-7b79-8031-89b4-d328625b67af" class="bulleted-list"><li style="list-style-type:square">여기서 QX​는 양자화된(low-bit) 값.</li></ul><ul id="1b0451cf-7b79-80a4-bfb9-d0ce5986f96e" class="bulleted-list"><li style="list-style-type:square">sX는 스케일링 팩터(Scaling Factor).</li></ul><ul id="1b0451cf-7b79-80c1-95ce-cbddde2ab0a5" class="bulleted-list"><li style="list-style-type:square">qmax는 최대 양자화 값(비트 수에 따라 달라짐).</li></ul><ul id="1b0451cf-7b79-8050-b9ea-c1794215549e" class="bulleted-list"><li style="list-style-type:square">4비트 부동소수점 양자화(4-bit FP)에서는 qmax=6임.</li></ul></li></ul></li></ul><ul id="1b0451cf-7b79-80c5-9775-c634072f35ae" class="bulleted-list"><li style="list-style-type:disc">양자화된 행렬 연산<ul id="1b0451cf-7b79-8057-ac15-ff08b78dcf7b" class="bulleted-list"><li style="list-style-type:circle">선형 계층(Linear Layer)에서 입력 X와 가중치 W가 있을 때, 연산을 양자화된 값으로 근사:</li></ul><figure id="1b0451cf-7b79-80dd-9121-e83dc97e1824" class="image"><a href="/images/2025-03-13-SVDQuant/image%206.png"><img style="width:681.953125px" src="/images/2025-03-13-SVDQuant/image%206.png"/></a></figure><ul id="1b0451cf-7b79-80ae-a8cd-f7b262009888" class="bulleted-list"><li style="list-style-type:circle">즉, 양자화된 텐서끼리 연산한 후, 스케일링 팩터 sX​sW​를 곱하여 다시 원래 값에 가깝게 복원함.</li></ul></li></ul><ul id="1b0451cf-7b79-80d1-b052-dcecf991c379" class="bulleted-list"><li style="list-style-type:disc">GPU에서 같은 비트폭(bit width)을 사용해야 하는 이유<ul id="1b0451cf-7b79-80ca-a560-dcfe04717a2d" class="bulleted-list"><li style="list-style-type:circle">최신 GPU에서는 입력(QX)과 가중치(QW)의 비트 수가 동일해야 연산 속도가 향상됨.</li></ul><ul id="1b0451cf-7b79-8064-b178-cb87f94b9d25" class="bulleted-list"><li style="list-style-type:circle">만약 QX와 QW의 비트 수가 다르면, 더 높은 비트 값으로 변환(upcast)되면서 속도 이점이 사라짐.</li></ul><ul id="1b0451cf-7b79-803f-89f6-d98f8324518e" class="bulleted-list"><li style="list-style-type:circle">예:<ul id="1b0451cf-7b79-8018-b775-ebf6f856d63c" class="bulleted-list"><li style="list-style-type:square">가중치(W)를 4비트로 양자화(W4)했지만, 활성화값(X)이 16비트(A16)라면?<br/>→ <br/>연산 시 W4가 A16으로 업캐스트(Upcast)되어 실제 속도 향상이 없음.<br/>→ 따라서, <br/>W4A4(가중치 4비트, 활성화값 4비트) 조합이 최적화된 방식.</li></ul></li></ul></li></ul><ul id="1b0451cf-7b79-809e-906f-f440c7bf5295" class="bulleted-list"><li style="list-style-type:disc">W4A4 양자화에서의 문제점: Outlier(이상치)<ul id="1b0451cf-7b79-80be-bb29-e1c5518f7f7d" class="bulleted-list"><li style="list-style-type:circle"><span style="border-bottom:0.05em solid">Diffusion 모델에서는 가중치(W)와 활성화값(X) 양쪽에서 Outlier(극단적인 값)가 많이 발생</span>함.</li></ul><ul id="1b0451cf-7b79-8072-9e8b-f717c17fd530" class="bulleted-list"><li style="list-style-type:circle">Outlier가 많으면 양자화 후 품질이 크게 저하됨.</li></ul><ul id="1b0451cf-7b79-80a2-8b1b-d1c39503f15c" class="bulleted-list"><li style="list-style-type:circle">기존 해결 방법:<ol type="1" id="1b0451cf-7b79-80da-b3d7-e72c02fd8a47" class="numbered-list" start="1"><li>Quantization-Aware Training (QAT)<ul id="1b0451cf-7b79-80c0-94fd-c1236f62165b" class="bulleted-list"><li style="list-style-type:disc">양자화를 고려하여 모델을 훈련하는 방식.</li></ul><ul id="1b0451cf-7b79-80ba-a0b4-dc239e4583f2" class="bulleted-list"><li style="list-style-type:disc">하지만, 100억 개 이상의 매개변수(예: FLUX.1 모델)를 조정하려면 계산 비용이 매우 큼.</li></ul></li></ol><ol type="1" id="1b0451cf-7b79-800f-8b55-c2d6731d8775" class="numbered-list" start="2"><li>Rotation 기법 (Ashkboos et al., 2024; Liu et al., 2024c)<ul id="1b0451cf-7b79-806d-9024-de1c2a7eca45" class="bulleted-list"><li style="list-style-type:disc">가중치와 활성화값을 회전(rotation)하여 Outlier를 줄이는 방법.</li></ul><ul id="1b0451cf-7b79-8061-a4cc-c78e3556a1d9" class="bulleted-list"><li style="list-style-type:disc">하지만, Diffusion 모델의 &quot;Adaptive Normalization Layer&quot;에서는 적용이 어려움.</li></ul><ul id="1b0451cf-7b79-808c-b886-edb6888d8679" class="bulleted-list"><li style="list-style-type:disc">이유:<ul id="1b0451cf-7b79-80c1-8ecc-fa11cd0eaf02" class="bulleted-list"><li style="list-style-type:circle">Adaptive Normalization은 실행 시간(runtime) 중에 새로운 가중치를 생성.</li></ul><ul id="1b0451cf-7b79-803f-b8e2-ed068f7059a3" class="bulleted-list"><li style="list-style-type:circle">따라서, 사전 계산된 회전 행렬을 적용할 수 없음.</li></ul><ul id="1b0451cf-7b79-80ca-8aac-e17d04732e11" class="bulleted-list"><li style="list-style-type:circle">실행 시간에 회전을 적용하면 연산량이 증가하여 속도가 느려짐.</li></ul></li></ul></li></ol></li></ul></li></ul><p id="1b0451cf-7b79-8096-a589-d7c6648b3890" class="">
</p><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1b4451cf-7b79-8085-8ee5-e934e7c60ee7"><div style="font-size:1.5em"><span class="icon">📢</span></div><div style="width:100%"><p id="1b4451cf-7b79-80fa-ae16-dabe1d521b64" class="">이상치(Outlier)가 있으면 어떻게 성능이 저하될까?</p><h3 id="1b4451cf-7b79-80fe-94e7-d6c1c716309f" class="">1. 스케일링 팩터 문제</h3><ul id="1b4451cf-7b79-804f-a6ef-c5755d9d2b88" class="bulleted-list"><li style="list-style-type:disc">양자화는 데이터의 전체 범위(min-max)를 고려해서 값을 조정해야 하는데, 이상치가 있으면 스케일링 팩터가 비정상적으로 커짐.</li></ul><ul id="1b4451cf-7b79-803f-8271-f348b513a8c1" class="bulleted-list"><li style="list-style-type:disc">대부분의 값은 작은 범위에 몰려 있는데, 한두 개의 큰 값(이상치) 때문에 스케일이 커지면 작은 값들이 모두 0 또는 동일한 값으로 매핑되는 문제가 생겨.<p id="1b4451cf-7b79-804e-9532-dc7ce450b758" class="">예제:</p><ul id="1b4451cf-7b79-80e1-9641-f5c7aebfdfd3" class="bulleted-list"><li style="list-style-type:circle">원래 가중치 값: <code>[-0.1, -0.05, 0.0, 0.05, 0.1, 5.0]</code> (이상치: 5.0)</li></ul><ul id="1b4451cf-7b79-807a-ab6f-c30c289a5090" class="bulleted-list"><li style="list-style-type:circle">이상치가 없을 때: <code>s_X = 0.1</code>, 범위를 <code>[-8, 7]</code>로 매핑 가능</li></ul><ul id="1b4451cf-7b79-803b-b3b5-ea43c717abf0" class="bulleted-list"><li style="list-style-type:circle">이상치(5.0)가 포함될 때: <code>s_X = 5.0</code>, 작은 값들은 모두 0이 되어 정보 손실 발생</li></ul></li></ul><h3 id="1b4451cf-7b79-8094-9ce0-d5349afe65d2" class="">2. 정보 손실 (Precision Loss)</h3><ul id="1b4451cf-7b79-80eb-95cd-c068a96a92d7" class="bulleted-list"><li style="list-style-type:disc">이상치를 고려해 전체 값을 조정하면, 나머지 대부분의 값이 매우 작은 차이를 가지는데도 동일한 양자화된 값으로 표현될 가능성이 높아.</li></ul><ul id="1b4451cf-7b79-8030-af7b-f405bbc7cb00" class="bulleted-list"><li style="list-style-type:disc">즉, 모델이 작은 변화(gradient 등)를 반영하지 못하고 표현력이 급격히 떨어짐.</li></ul><h3 id="1b4451cf-7b79-80ac-ae9d-fb3363b903fa" class="">3. 활성화(Activation) 이상치로 인해 연산량 증가</h3><ul id="1b4451cf-7b79-805f-bd60-fb3d7522aa91" class="bulleted-list"><li style="list-style-type:disc">이상치가 있으면 양자화된 값을 다시 부동소수점으로 변환할 때 FP32(32-bit)로 변환하는 경우가 많아, 결국 연산 최적화가 깨짐.</li></ul><ul id="1b4451cf-7b79-8001-8d79-c6384bfb4404" class="bulleted-list"><li style="list-style-type:disc">특히 Transformer 기반 모델에서는 Self-Attention 연산이 크기 때문에 활성화값(Activation)의 이상치는 메모리 사용량과 연산량 증가로 이어질 수 있음.</li></ul><p id="1b4451cf-7b79-80fc-8265-eab3d51e351d" class="">
</p></div></figure><h1 id="1b0451cf-7b79-80b7-93aa-de0c77515d05" class="">4 Method</h1><p id="1b4451cf-7b79-80cf-b998-e21b580b4344" class="">
</p><h2 id="1b4451cf-7b79-8054-a2e2-fe6883e5e369" class="">4.1 PROBLEM FORMULATION</h2><p id="1b5451cf-7b79-8063-8579-df66165101b7" class="">양자화의 오류를 다음과 같이 정의됨.</p><figure id="1b5451cf-7b79-8040-8783-e513d77e939a" class="image" style="text-align:left"><a href="/images/2025-03-13-SVDQuant/image%207.png"><img style="width:384px" src="/images/2025-03-13-SVDQuant/image%207.png"/></a></figure><p id="1b5451cf-7b79-80eb-82da-e0b53f626858" class="">원래 행렬 곱셈 XW와 양자화된 값으로 연산한 Q(X)Q(W)의 차이를 측정하는 값</p><p id="1b5451cf-7b79-80c5-a6c4-d376626e90d8" class="">
</p><p id="1b5451cf-7b79-80a7-8b1d-d812b025cb02" class="">좀 더 세분화</p><figure id="1b5451cf-7b79-80e9-9c72-f26b95afe3c2" class="image"><a href="/images/2025-03-13-SVDQuant/image%208.png"><img style="width:709.9140625px" src="/images/2025-03-13-SVDQuant/image%208.png"/></a></figure><figure id="1b5451cf-7b79-80ec-b4d7-f42587746b70" class="image"><a href="/images/2025-03-13-SVDQuant/image%209.png"><img style="width:576px" src="/images/2025-03-13-SVDQuant/image%209.png"/></a></figure><p id="1b5451cf-7b79-807f-bda3-ed4a816512ee" class="">
</p><p id="1b5451cf-7b79-801c-b016-f87be0a50e7e" class="">
</p><p id="1b5451cf-7b79-8013-a4f5-d943e040ae3d" class="">
</p><h2 id="1b4451cf-7b79-80fd-b19f-d41847604e12" class="">4.2 SVDQUANT: ABSORBING OUTLIERS VIA LOW-RANK BRANCH</h2><hr id="1b4451cf-7b79-803d-930f-ed86b403b9d8"/><h3 id="1b4451cf-7b79-8071-8cb0-d42a6cd514bc" class="">Migrate outliers from activation to weight</h3><figure id="1b4451cf-7b79-8084-9053-da5bac137308" class="image"><a href="/images/2025-03-13-SVDQuant/image%2010.png"><img style="width:709.984375px" src="/images/2025-03-13-SVDQuant/image%2010.png"/></a></figure><p id="1b4451cf-7b79-80d2-a144-da65900cc7b1" class="">After Smoothing 부분이 기존 기법인데, 단점이 있음</p><ul id="1b5451cf-7b79-80d0-a5f5-f5a34450beae" class="bulleted-list"><li style="list-style-type:disc">✅ Activation(X)의 이상치를 없애는 것은 성공했지만,</li></ul><ul id="1b5451cf-7b79-8048-86c2-d18bbad1a64d" class="bulleted-list"><li style="list-style-type:disc">❌ 대신 Weight(W)의 이상치가 증가하는 문제가 발생.</li></ul><ul id="1b5451cf-7b79-80f2-8ef2-d9e907c8ae82" class="bulleted-list"><li style="list-style-type:disc">결과적으로, 전체적인 양자화 오류를 줄이려는 목적이 제대로 달성되지 않음.</li></ul><p id="1b5451cf-7b79-806f-846b-e2b09fa70859" class="">
</p><h3 id="1b5451cf-7b79-8041-8c84-ecfc5e5922cb" class="block-color-yellow_background">Absorb magnified weight outliers with a low-rank branch.</h3><p id="1b5451cf-7b79-80a3-b791-c78e79c2de89" class="">Weight를 바로 4-bit로 양자화하지 않고, Low-Rank Component를 따로 분리해서 이상치를 흡수하는 전략</p><p id="1b5451cf-7b79-803a-8c88-c21d031d57ed" class="">
</p><figure id="1b5451cf-7b79-8003-a71b-e42214fa7cc5" class="image"><a href="/images/2025-03-13-SVDQuant/image%2011.png"><img style="width:709.9609375px" src="/images/2025-03-13-SVDQuant/image%2011.png"/></a></figure><figure id="1b5451cf-7b79-803e-a933-e3731865ed34" class="image" style="text-align:left"><a href="/images/2025-03-13-SVDQuant/image%2012.png"><img style="width:384px" src="/images/2025-03-13-SVDQuant/image%2012.png"/></a></figure><p id="1b5451cf-7b79-80e8-a1bd-c55589a3cbe6" class="">
</p><p id="1b5451cf-7b79-806a-a32d-d8f6b89e23ca" class="">
</p><h3 id="1b9451cf-7b79-80f9-a6a0-ebdb6cb7faac" class="">SVD(Singular Value Decomposition, 특이값 분해)</h3><ul id="1b5451cf-7b79-8064-a4f7-cf2ec265d91f" class="bulleted-list"><li style="list-style-type:disc">원래 행렬 크기가 m×n이면, 직접 곱하면 연산량이 O(mn).</li></ul><ul id="1b5451cf-7b79-8023-aa46-f31fbbc72d66" class="bulleted-list"><li style="list-style-type:disc">하지만 SVD로 Rank r만 유지하면 연산량이 O(mr+rn)로 줄어듦.</li></ul><figure id="1b9451cf-7b79-8030-93b9-d7e0d4fab927" class="image" style="text-align:left"><a href="/images/2025-03-13-SVDQuant/image%2013.png"><img style="width:384px" src="/images/2025-03-13-SVDQuant/image%2013.png"/></a></figure><p id="1b9451cf-7b79-8070-b01a-dba731140a46" class=""> 포인트는 대각 원소(특이값, Singular Values)</p><ul id="1b9451cf-7b79-803d-9ac5-dc82d55eef83" class="bulleted-list"><li style="list-style-type:disc">큰 특이값들은 중요한 정보(패턴)를 나타냄.</li></ul><ul id="1b9451cf-7b79-80c5-8538-dd97452dbdbf" class="bulleted-list"><li style="list-style-type:disc">작은 특이값들은 노이즈(이상치 포함)를 나타낼 가능성이 높음.</li></ul><p id="1b9451cf-7b79-80f1-8f7b-ebea3afbdaa7" class="">
</p><h3 id="1b9451cf-7b79-80bf-8f75-ed07a8408afc" class="">Low-Rank 분해 </h3><figure id="1b5451cf-7b79-80be-b5c6-cb35a1699613" class="image"><a href="/images/2025-03-13-SVDQuant/image%2014.png"><img style="width:709.96875px" src="/images/2025-03-13-SVDQuant/image%2014.png"/></a></figure><p id="1b9451cf-7b79-808c-b9c0-cb57b5a305e6" class=""><mark class="highlight-yellow_background">대각 원소 중에서 상위 r개의 특이값만 유지</mark>하여, 가장 중요한 정보만 포함하는 L1,L2를 생성. </p><p id="1b9451cf-7b79-806e-a1d7-d18db5ec03f3" class="">→ 따로 16-bit 연산</p><p id="1b9451cf-7b79-8045-88fa-c4f5633418b3" class=""><mark class="highlight-yellow_background">남은 부분(작은 특이값)</mark> → R으로 분리 </p><p id="1b9451cf-7b79-8087-8635-fb1ab461f555" class="">→ 4-bit 연산</p><p id="1b9451cf-7b79-804b-82ac-f9668ecca85d" class="">
</p><p id="1b9451cf-7b79-8031-afcc-dcf955a24738" class="">→ 작은 특이값만 남았으므로 R을 4-bit로 양자화하더라도 정보 손실이 크게 줄어듦</p><p id="1b9451cf-7b79-80ea-9676-f298086a02f3" class="">
</p><p id="1b4451cf-7b79-80bd-9072-cf28b76dcc8c" class="">
</p><h2 id="1b4451cf-7b79-80a9-8bd2-e53684d44a92" class="">4.3 NUNCHAKU: Fusing Low-Rank and Low-Bit Branch Kernels</h2><figure id="1b4451cf-7b79-80c6-9315-d2f2b998bf78" class="image"><a href="/images/2025-03-13-SVDQuant/image%2015.png"><img style="width:709.9921875px" src="/images/2025-03-13-SVDQuant/image%2015.png"/></a></figure><hr id="1b4451cf-7b79-805c-98bf-dd5f162d3829"/><h3 id="1b4451cf-7b79-8020-8142-d7133e5e1aa9" class="">Low-Rank Branch에서 발생하는 성능 저하 문제</h3><ul id="1b4451cf-7b79-801b-9e26-eda745a7017c" class="bulleted-list"><li style="list-style-type:disc">QKV Projection과 같은 연산에서는 Low-Rank Branch가 L2 캐시를 초과하면서 DRAM에서 데이터를 불러와야 함.</li></ul><ul id="1b4451cf-7b79-8001-8b02-ec5e1da8f2e8" class="bulleted-list"><li style="list-style-type:disc">이는 메모리 접근 비용이 증가하여 연산 속도가 떨어지는 원인.</li></ul><ul id="1b4451cf-7b79-8014-86f7-f53ae21c2d7c" class="bulleted-list"><li style="list-style-type:disc">Figure 6(a)에서 보듯이, Low-Rank Branch는 전체 4-bit 연산 지연의 50%를 차지.</li></ul><hr id="1b4451cf-7b79-8034-9f73-cadb388dcc08"/><h3 id="1b4451cf-7b79-80f4-b396-f14e1b10f544" class="">NUNCHAKU: 해결 방법</h3><ul id="1b4451cf-7b79-80e5-9b08-df1ed98d4802" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-orange_background">논문에서는 </mark><mark class="highlight-orange_background">Low-Rank Branch와 Low-Bit Branch의 연산을 하나로 합쳐(fusing) 메모리 접근을 줄이는 방법을 제안</mark><mark class="highlight-orange_background">.</mark></li></ul><ul id="1b4451cf-7b79-8040-9f97-f897c659eee9" class="bulleted-list"><li style="list-style-type:disc">Figure 6(b)에서 보듯이, 두 개의 Kernel을 합쳐서 데이터를 공유함:<ol type="1" id="1b4451cf-7b79-8065-a3fc-c7ad225403dc" class="numbered-list" start="1"><li>Down Projection 연산을 Quantization Kernel과 합침.</li></ol><ol type="1" id="1b4451cf-7b79-805d-8fad-da3447dd9db0" class="numbered-list" start="2"><li>Up Projection 연산을 4-bit 연산 Kernel과 합침.</li></ol></li></ul><ul id="1b4451cf-7b79-80cc-989b-f3f2fb17990b" class="bulleted-list"><li style="list-style-type:disc">이를 통해 Low-Rank Branch가 Low-Bit Branch와 활성화값을 공유할 수 있어, 추가적인 메모리 접근을 제거.</li></ul><ul id="1b4451cf-7b79-80db-a8f1-d827859a6703" class="bulleted-list"><li style="list-style-type:disc">결과적으로, Kernel 호출 횟수가 절반으로 줄어들어 속도 개선 효과가 있음.</li></ul><p id="1b4451cf-7b79-8034-8a61-ebb13c9dbf45" class="">
</p><p id="1b4451cf-7b79-80b1-b8fd-eea92e48f82b" class="">
</p><p id="1b4451cf-7b79-8033-8767-e918cb5b1aef" class="">
</p><p id="1b0451cf-7b79-800d-99d2-e6580a457e9f" class="">
</p><h1 id="1b0451cf-7b79-800c-a182-fe595644a631" class="">5 Experiments</h1><h3 id="1b9451cf-7b79-805f-91df-eef69e1b0c6b" class="">Benchmark models</h3><table id="1b9451cf-7b79-80ae-8b87-da042df55667" class="simple-table"><tbody><tr id="1b9451cf-7b79-8030-a28d-faec444a75d8"><td id="aUJ|" class="">Model</td><td id="LJ;p" class="">Architecture</td><td id="m]Yg" class="">Parameters</td><td id="?hTZ" class="">Special Features</td></tr><tr id="1b9451cf-7b79-80bc-9d21-cad69757e339"><td id="aUJ|" class="">FLUX.1-dev</td><td id="LJ;p" class="">DiT</td><td id="m]Yg" class="">12B</td><td id="?hTZ" class="">50-step guidance-distilled</td></tr><tr id="1b9451cf-7b79-8022-88d2-c16c2e358ced"><td id="aUJ|" class="">FLUX.1-schnell</td><td id="LJ;p" class="">DiT</td><td id="m]Yg" class="">12B</td><td id="?hTZ" class="">4-step timestep-distilled</td></tr><tr id="1b9451cf-7b79-804f-98b1-e428f888fbd7"><td id="aUJ|" class="">PixArt-Σ</td><td id="LJ;p" class="">DiT</td><td id="m]Yg" class="">600M</td><td id="?hTZ" class="">20-step default</td></tr><tr id="1b9451cf-7b79-80b8-98f9-d2d48c452321"><td id="aUJ|" class="">SANA</td><td id="LJ;p" class="">DiT</td><td id="m]Yg" class="">1.6B</td><td id="?hTZ" class="">32× compression autoencoder, Linear Attention</td></tr><tr id="1b9451cf-7b79-80e1-8776-f194ffe206dc"><td id="aUJ|" class="">SDXL</td><td id="LJ;p" class="">UNet</td><td id="m]Yg" class="">2.6B</td><td id="?hTZ" class="">30-step</td></tr></tbody></table><h3 id="1b9451cf-7b79-8091-b966-f2432aefb45e" class="">Baselines Quantization</h3><table id="1b9451cf-7b79-80f1-9730-f6c63a349b46" class="simple-table"><tbody><tr id="1b9451cf-7b79-80ab-b8f9-e75c411763db"><td id="&lt;lV]" class="">Method</td><td id="?OWR" class="">Description</td><td id="yunN" class="">Usage in Benchmarking</td></tr><tr id="1b9451cf-7b79-806a-a156-f635b46b71a1"><td id="&lt;lV]" class="">NF4<br/>(4-bit NormalFloat)<br/></td><td id="?OWR" class="">Optimized 4-bit weight-only quantization assuming normal distribution</td><td id="yunN" class="">Used as a weight-only quantization baseline for FLUX.1</td></tr><tr id="1b9451cf-7b79-80ad-956d-d8053cca98b5"><td id="&lt;lV]" class="">ViDiT-Q</td><td id="?OWR" class="">Per-token quantization + smoothing to reduce outliers</td><td id="yunN" class="">Achieves lossless 8-bit quantization on PixArt-Σ</td></tr><tr id="1b9451cf-7b79-80e2-b74e-f201b50a1cbd"><td id="&lt;lV]" class="">MixDQ</td><td id="?OWR" class="">Detects outliers in text embeddings and protects them with 16-bit pre-computation</td><td id="yunN" class="">Enables W4A8 quantization with minimal performance drop on SDXL-Turbo</td></tr><tr id="1b9451cf-7b79-807a-98b4-d434a27793a5"><td id="&lt;lV]" class="">TensorRT</td><td id="?OWR" class="">Industry-standard PTQ toolkit for 8-bit quantization</td><td id="yunN" class="">Uses smoothing + percentile calibration over specific timesteps</td></tr></tbody></table><figure id="1b3451cf-7b79-80c7-9c88-ea2289a5f38f" class="image"><a href="/images/2025-03-13-SVDQuant/image%2016.png"><img style="width:709.9921875px" src="/images/2025-03-13-SVDQuant/image%2016.png"/></a></figure><figure id="1b9451cf-7b79-8048-a239-e2d6aa27f92e" class="image"><a href="/images/2025-03-13-SVDQuant/image%2017.png"><img style="width:709.984375px" src="/images/2025-03-13-SVDQuant/image%2017.png"/></a></figure><figure id="1b3451cf-7b79-80b2-b150-d0f26013425e" class="image"><a href="/images/2025-03-13-SVDQuant/image%2018.png"><img style="width:709.9765625px" src="/images/2025-03-13-SVDQuant/image%2018.png"/></a></figure><figure id="1b9451cf-7b79-800a-bb07-fb2185e3e8d3" class="image"><a href="/images/2025-03-13-SVDQuant/image%2019.png"><img style="width:709.9921875px" src="/images/2025-03-13-SVDQuant/image%2019.png"/></a></figure><figure id="1b9451cf-7b79-80f9-b9e7-e0fb7a02557f" class="image"><a href="/images/2025-03-13-SVDQuant/image%2020.png"><img style="width:709.9921875px" src="/images/2025-03-13-SVDQuant/image%2020.png"/></a></figure><p id="1b3451cf-7b79-8016-a721-eb2e6bdb3df9" class="">
</p><p id="1b3451cf-7b79-801c-8b7c-decce0570ec6" class="">
</p><h1 id="1b3451cf-7b79-8069-a8d8-cce825d2197e" class="">Limitation</h1><p id="1b9451cf-7b79-80d0-8c8f-f787946d192e" class="">엄청난 기술이고 기술면에서는 한계가 없다고 생각함. </p><p id="1b9451cf-7b79-80e3-8195-fd5a412c5cb4" class="">하지만, 굳이 한계점을 뽑자면, Song Han이 NVIDIA에서도 연구를 진행하기 때문에 NVIDIA chip만을 위해서 코드를 짰고, 이에 최적화되어있다.</p><p id="1b9451cf-7b79-8001-b0c1-c88078f30bf9" class="">심지어 CUDA 12.2 이상에서만 작동 가능해서 내 학교 서버로 돌려보려고 했는데, GPU Driver version이 낮아서 안돌아가더라.</p><figure id="1b9451cf-7b79-80b1-90ea-e3f4976a02bc" class="image"><a href="/images/2025-03-13-SVDQuant/image%2021.png"><img style="width:709.984375px" src="/images/2025-03-13-SVDQuant/image%2021.png"/></a></figure><p id="1b5451cf-7b79-809a-be3b-d50cf256f252" class="">물론 NVIDIA chip에서 극한의 최적화를 위해서 였지만, 다른 GPU 장비에서는 이를 사용할 수 없다.</p><p id="1b9451cf-7b79-8099-b811-d3f49b2751c6" class="">같은 방식을 다른 GPU 장비와 Mobile edge device들에 적용한다면 좋을 것이다.</p><p id="1b9451cf-7b79-8034-8ef0-db4f0ee73b51" class="">
</p><p id="1b9451cf-7b79-80af-aa03-f74a76e2d116" class="">
</p><p id="1b9451cf-7b79-80a9-9db0-cc9a176288ba" class="">
</p><h1 id="1b9451cf-7b79-80a2-91bb-dd9012fcaa7b" class="block-color-red_background">메모리랑 추론시간만 줄인게 아닌가? 정확도를 왜 언급?</h1><h3 id="1b9451cf-7b79-80bc-808a-ff3da0234b8d" class="">Memory, Latency?</h3><p id="1b9451cf-7b79-806e-9095-e10f17ed8580" class="">아래 그림과 같이 SVD양자화로 인해 메모리와 latency 이점을 얻은게 포인트 아닌가?</p><figure id="1b9451cf-7b79-80a2-8cd8-fa65283b56c1" class="image"><a href="/images/2025-03-13-SVDQuant/image%2022.png"><img style="width:695px" src="/images/2025-03-13-SVDQuant/image%2022.png"/></a></figure><p id="1b9451cf-7b79-80c4-b42e-c30f72c5bfd2" class="block-color-yellow_background">비교가 기존 16bit / W4A16 / W4A4(SVD) 였기 때문에 큰 차이를 보여준 것 같다.</p><p id="1b9451cf-7b79-8055-a534-eb7be3a7153c" class="">Inference time과 Memory 줄인게 포인트인줄 알았는데 완전 잘못 생각한것 같기도 합니다.</p><p id="1b9451cf-7b79-808f-8cb9-c51ec1d5b09c" class="">
</p><p id="1b9451cf-7b79-8056-90ad-f8224f8d116d" class="block-color-orange_background">물론 Outlier때문에 32bit로 처리했던 부분들이 없어지고 16bit로 low rank로 따로 빼니까 향상은 됐을 것이지만, 제 생각에는 SVD 없는 W4A4랑 비교했다면, Memory와 추론시간이 3배 이상 차이나지는 않을 것입니다.</p><figure id="1b9451cf-7b79-8020-b2df-cefc97fa8752" class="image"><a href="/images/2025-03-13-SVDQuant/image%2023.png"><img style="width:694.984375px" src="/images/2025-03-13-SVDQuant/image%2023.png"/></a></figure><h3 id="1b9451cf-7b79-8054-85b1-f77ceb4c8f23" class="">정확도?</h3><p id="1b9451cf-7b79-80ff-9ed1-c193dde950ec" class="">기존 양자화에서 정확도를 유지하는 부분이 아래 그림처럼 많이 떨어졌었다.</p><p id="1b9451cf-7b79-803b-b04c-d868ac6d01cd" class="">(두번째가 기존 양자화 기법, 심지어 W4A4가 아닌 W4A16인데도 더 떨어지는 모습을 보임)</p><figure id="1b9451cf-7b79-80c7-b6e8-fcd54c9ae799" class="image" style="text-align:left"><a href="/images/2025-03-13-SVDQuant/image%2024.png"><img style="width:432px" src="/images/2025-03-13-SVDQuant/image%2024.png"/></a></figure><h3 id="1b9451cf-7b79-80de-8586-fa234a4e0e1c" class="">보존 할 수 있었던 이유 → Low-Rank Branch</h3><ul id="1b9451cf-7b79-8022-be65-ee52029ad106" class="bulleted-list"><li style="list-style-type:disc">기존 4-bit 양자화 방식에서는 Weight 전체를 4-bit로 변환하므로 정보 손실이 큼.</li></ul><ul id="1b9451cf-7b79-8064-8718-d50903d3bb1f" class="bulleted-list"><li style="list-style-type:disc">SVDQuant는 Weight를 Low-Rank Component (L1L2) 와 잔여 (R)로 분해한다.</li></ul><figure id="1b9451cf-7b79-80f3-a3ac-e03cce681659" class="image" style="text-align:left"><a href="/images/2025-03-13-SVDQuant/image%2025.png"><img style="width:432px" src="/images/2025-03-13-SVDQuant/image%2025.png"/></a></figure><ul id="1b9451cf-7b79-80c4-9f67-e284236a0719" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-yellow_background"><span style="border-bottom:0.05em solid">L1L2는 16-bit precision으로 유지 → 중요한 정보는 고정밀도로 남겨둠.</span></mark></li></ul><ul id="1b9451cf-7b79-806b-b0d5-cdb0a5a8cda9" class="bulleted-list"><li style="list-style-type:disc">잔여 R만 4-bit로 양자화하여 정보 손실을 최소화함</li></ul><ul id="1b9451cf-7b79-80b4-9b57-c478678e7d50" class="bulleted-list"><li style="list-style-type:disc">한 번만 Low-Rank 분해하는 것이 아니라, 반복적으로 R을 최적화하여 양자화 오류를 최소화</li></ul><p id="1b9451cf-7b79-8008-addf-df773f81d36d" class="">
</p><h3 id="1b9451cf-7b79-808d-9c6f-f3db6c7574aa" class="">quantization error</h3><figure id="1b9451cf-7b79-80ba-8e79-c905a73bc730" class="image"><a href="/images/2025-03-13-SVDQuant/image%2026.png"><img style="width:694.9453125px" src="/images/2025-03-13-SVDQuant/image%2026.png"/></a></figure><p id="1b9451cf-7b79-804e-a8c5-fb4bb7a0d47c" class="">이 식에서 L1L2+R로 분해함으로서 Quantization을 진행하면서 발생하는 Error를 최대한 줄인거임.</p><p id="1b9451cf-7b79-8018-b00c-d9eb56f9faeb" class="">
</p><p id="1b9451cf-7b79-80cf-8f38-d29ef01a5203" class="block-color-blue_background">Quantization 자체가 Outlier로 인해서 정확도를 떨어뜨릴 수 밖에 없는데, 이를 최대한 보존했다는 점이 엄청난 연구인 것임!!!!</p><h1 id="1b9451cf-7b79-804d-9f66-e41d84df02ba" class="block-color-red_background">SVD를 LLM에 써도 되는가? 왜 Diffusion으로 논문을?</h1><p id="1b9451cf-7b79-80e0-aef1-fbbf56042c83" class="block-color-teal_background">기존 제 생각 : 적용 할 수는 있을 것 같으나, LLM은 병목현상이 <span style="border-bottom:0.05em solid">무거운 모델을 불러오는 과정</span>에서 나타나므로 뒤에 연산을 줄여도 Diffusion만큼 큰 효과는 나타날 지 모르겠습니다. (<span style="border-bottom:0.05em solid">Diffusion은 연산이 병목</span>임)</p><p id="1b9451cf-7b79-80df-ace1-c33706c16ab7" class="">
</p><p id="1b9451cf-7b79-8012-9ba8-ed02e08ebd53" class="">물론 기존 제 생각도 찾아보니 맞는 것 같으나, 실제로 활용한다면, 바로 위 질문에서 다뤘던 정확도 향상에도 도움이 되기 때문에, 오히려 정확도 부분에서 도움이 될 것 같습니다.</p><p id="1b9451cf-7b79-8008-81c3-e61e76c68f60" class="">
</p><h3 id="1b9451cf-7b79-804f-8ccf-d771d8e662af" class="">기존 양자화</h3><ul id="1b9451cf-7b79-80d7-9093-d63eb71b6a6f" class="bulleted-list"><li style="list-style-type:disc">GPTQ → Post-training quantization 방식, Weight만 4-bit 변환.</li></ul><ul id="1b9451cf-7b79-8014-83b1-d7f2d7876902" class="bulleted-list"><li style="list-style-type:disc">AWQ → Weight 중요도를 분석해 선택적으로 4-bit 변환.</li></ul><ul id="1b9451cf-7b79-8030-95d4-fd6529cde607" class="bulleted-list"><li style="list-style-type:disc">SmoothQuant → Activation 이상치를 Weight로 이동시켜 양자화 오류를 줄이는 방식.</li></ul><p id="1b9451cf-7b79-801c-a6b9-ddd0e4110575" class="">
</p><h3 id="1b9451cf-7b79-803b-8cf3-cb358fec4a1a" class="">SVDQuant 사용한다면?</h3><p id="1b9451cf-7b79-8076-af24-d2dcf281b3da" class="">Weight를 Low-Rank(16-bit) + Residual(4-bit)로 나누어 중요한 정보는 유지하면서 압축하므로</p><p id="1b9451cf-7b79-8076-94a5-f961585b2b47" class="">Diffusion처럼 정확도 향상에 도움이 될듯 !!!</p><p id="1b9451cf-7b79-80d3-b36d-e9093fc70da1" class="">
</p><p id="1b9451cf-7b79-80d4-ad96-f1468b285f55" class="">
</p><p id="1b9451cf-7b79-8010-846c-d9685870a1ff" class="">
</p><h1 id="1b9451cf-7b79-8050-a9dd-d24ae3c8c99a" class="">For an explanation from the author, Song Han</h1><figure id="1b9451cf-7b79-80aa-9611-df893eea3b07" class="image"><a href="/images/2025-03-13-SVDQuant/image%2027.png"><img style="width:709.9921875px" src="/images/2025-03-13-SVDQuant/image%2027.png"/></a></figure><p id="1b9451cf-7b79-80a6-83f7-ce2fd3f35fb4" class=""><a href="https://www.youtube.com/watch?v=nYujDH9r69s&amp;t=1s">Youtube [Introduction to SVDQuant for 4-bit Diffusion Models]</a></p><p id="1b9451cf-7b79-801d-bc12-e4601f945f32" class="">
</p><p id="1b9451cf-7b79-807f-8071-f4a9689437c5" class="">
</p><h1 id="1b9451cf-7b79-80d7-8244-d4fca4f1d07e" class="">Demo</h1><p id="1b9451cf-7b79-80ce-9933-f7b6ca6c337c" class=""><a href="https://hanlab.mit.edu/projects/svdquant">https://hanlab.mit.edu/projects/svdquant</a></p><figure id="1b9451cf-7b79-8021-aaef-dd4cf2ca4a7b" class="image"><a href="/images/2025-03-13-SVDQuant/image%2028.png"><img style="width:709.9921875px" src="/images/2025-03-13-SVDQuant/image%2028.png"/></a></figure></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>